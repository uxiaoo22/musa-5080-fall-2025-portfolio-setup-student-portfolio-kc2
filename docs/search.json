[
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html",
    "href": "weekly-notes/weekotes qmd.html",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We observe data such as counties, income, population, education, etc.\nWe believe there’s some relationship between these variables.\nStatistical learning = a set of approaches for estimating that relationship."
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#the-general-problem",
    "href": "weekly-notes/weekotes qmd.html#the-general-problem",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We observe data such as counties, income, population, education, etc.\nWe believe there’s some relationship between these variables.\nStatistical learning = a set of approaches for estimating that relationship."
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#formalizing-the-relationship",
    "href": "weekly-notes/weekotes qmd.html#formalizing-the-relationship",
    "title": "Week 5 Notes",
    "section": "Formalizing the Relationship",
    "text": "Formalizing the Relationship\nFor any quantitative response Y and predictors X₁, X₂, … Xₚ:\n\\[\nY = f(X) + \\epsilon\n\\]\nWhere:\n\nf = the systematic information X provides about Y\n\nε = random error (irreducible)"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#what-is-f",
    "href": "weekly-notes/weekotes qmd.html#what-is-f",
    "title": "Week 5 Notes",
    "section": "What is f?",
    "text": "What is f?\nf represents the true relationship between predictors and the outcome.\n\nIt’s fixed but unknown — what we’re trying to estimate.\n\nDifferent X values produce different Y values through f.\n\nExample:\n\nY = median income\n\nX = population, education, poverty rate\n\nf = the way these factors systematically relate to income"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#why-estimate-f",
    "href": "weekly-notes/weekotes qmd.html#why-estimate-f",
    "title": "Week 5 Notes",
    "section": "Why Estimate f?",
    "text": "Why Estimate f?\nTwo main reasons:\n\nPrediction\n\nEstimate Y for new observations\n\nFocus on accuracy, not interpretation\n\nInference\n\nUnderstand how X affects Y\n\nIdentify which predictors matter\n\nFocus on interpretation"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#how-do-we-estimate-f",
    "href": "weekly-notes/weekotes qmd.html#how-do-we-estimate-f",
    "title": "Week 5 Notes",
    "section": "How Do We Estimate f?",
    "text": "How Do We Estimate f?\n\nParametric Methods\n\nAssume a specific functional form (e.g., linear)\nEasier to interpret and compute\nFocus of this week’s session\n\n\n\nNon-Parametric Methods\n\nMake fewer assumptions\n\nMore flexible but require more data and are harder to interpret\n\nKey difference:\n- Parametric (blue): Assume f is linear → estimate coefficients\n- Non-parametric (green): Let data determine the shape of f"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#parametric-approach-linear-regression",
    "href": "weekly-notes/weekotes qmd.html#parametric-approach-linear-regression",
    "title": "Week 5 Notes",
    "section": "Parametric Approach: Linear Regression",
    "text": "Parametric Approach: Linear Regression\nAssumption: Relationship between X and Y is linear\n\\[\nY ≈ β₀ + β₁X₁ + β₂X₂ + … + βₚXₚ\n\\]\nWe estimate the β coefficients using Ordinary Least Squares (OLS)."
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#why-linear-regression",
    "href": "weekly-notes/weekotes qmd.html#why-linear-regression",
    "title": "Week 5 Notes",
    "section": "Why Linear Regression?",
    "text": "Why Linear Regression?\nAdvantages - Simple and interpretable\n- Foundation for many other models\n- Performs well in practice\nLimitations - Assumes linearity\n- Sensitive to outliers\n- Relies on several assumptions (we’ll check them)"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#prediction-vs-inference",
    "href": "weekly-notes/weekotes qmd.html#prediction-vs-inference",
    "title": "Week 5 Notes",
    "section": "Prediction vs Inference",
    "text": "Prediction vs Inference\nInference - “Does education affect income?”\n- Focus: coefficients, significance, mechanisms\nPrediction - “What’s County Y’s income?”\n- Focus: accuracy, prediction intervals\nToday: we’ll do both, but emphasize prediction."
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#example-prediction-policy",
    "href": "weekly-notes/weekotes qmd.html#example-prediction-policy",
    "title": "Week 5 Notes",
    "section": "Example: Prediction (Policy)",
    "text": "Example: Prediction (Policy)\nGovernment use case:\n\nCensus may miss people in hard-to-count areas\n\nPredict income or population for planning resources\n\nEven if the model doesn’t explain why, good predictions help policy."
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#example-inference-research",
    "href": "weekly-notes/weekotes qmd.html#example-inference-research",
    "title": "Week 5 Notes",
    "section": "Example: Inference (Research)",
    "text": "Example: Inference (Research)\nResearch use case:\n- Understanding gentrification\n- Which neighborhood characteristics explain income change?\n- How much does education matter vs. proximity to downtown?\n- Are policy interventions associated with outcomes?"
  },
  {
    "objectID": "weekly-notes/weekotes qmd.html#connection-to-week-2-algorithmic-bias",
    "href": "weekly-notes/weekotes qmd.html#connection-to-week-2-algorithmic-bias",
    "title": "Week 5 Notes",
    "section": "Connection to Week 2: Algorithmic Bias",
    "text": "Connection to Week 2: Algorithmic Bias\nRemember the healthcare algorithm that discriminated?\n\nIt predicted healthcare needs using costs as a proxy.\n\nTechnically accurate (good R²) but ethically biased.\n\nLesson: A good fit ≠ a fair model."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes",
    "section": "",
    "text": "[List main concepts from lecture]\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 4 Notes",
    "section": "",
    "text": "[List main concepts from lecture]\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 4 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4 Notes",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I’ll apply this knowledge]"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We observe data such as counties, income, population, and education.\nWe believe there’s some relationship between these variables.\nStatistical learning refers to the set of methods for estimating that relationship."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#the-general-problem",
    "href": "weekly-notes/week-05-notes.html#the-general-problem",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We observe data such as counties, income, population, and education.\nWe believe there’s some relationship between these variables.\nStatistical learning refers to the set of methods for estimating that relationship."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#formalizing-the-relationship",
    "href": "weekly-notes/week-05-notes.html#formalizing-the-relationship",
    "title": "Week 5 Notes",
    "section": "Formalizing the Relationship",
    "text": "Formalizing the Relationship\nFor any quantitative response Y and predictors X₁, X₂, … Xₚ:\n\\[\nY = f(X) + \\epsilon\n\\]\nWhere: - f = the systematic information X provides about Y\n- ε = random error (irreducible)"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#what-is-f",
    "href": "weekly-notes/week-05-notes.html#what-is-f",
    "title": "Week 5 Notes",
    "section": "What is f?",
    "text": "What is f?\nf represents the true relationship between predictors and the outcome.\nIt’s fixed but unknown — the core object we aim to estimate.\nExample: - Y = median income\n- X = population, education, poverty rate\n- f = how these factors systematically relate to income"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#why-estimate-f",
    "href": "weekly-notes/week-05-notes.html#why-estimate-f",
    "title": "Week 5 Notes",
    "section": "Why Estimate f?",
    "text": "Why Estimate f?\nTwo key purposes of statistical learning:\n\nPrediction\n\nEstimate Y for new or missing observations\n\nFocus on accuracy rather than explanation\n\nInference\n\nUnderstand how X affects Y\n\nIdentify which predictors are most important\n\nFocus on interpreting relationships"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#how-do-we-estimate-f",
    "href": "weekly-notes/week-05-notes.html#how-do-we-estimate-f",
    "title": "Week 5 Notes",
    "section": "How Do We Estimate f?",
    "text": "How Do We Estimate f?\n\nParametric Methods\n\nAssume a specific form for f (e.g., linear)\nReduce the problem to estimating parameters\nEasier to interpret but limited by assumptions\n\n\n\nNon-Parametric Methods\n\nMake fewer assumptions about f\nMore flexible but harder to interpret and require more data\n\nKey difference:\nParametric = structure first, then estimate\nNon-parametric = data first, let the shape emerge"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#parametric-approach-linear-regression",
    "href": "weekly-notes/week-05-notes.html#parametric-approach-linear-regression",
    "title": "Week 5 Notes",
    "section": "Parametric Approach: Linear Regression",
    "text": "Parametric Approach: Linear Regression\nWe assume a linear relationship between X and Y:\n\\[\nY ≈ β₀ + β₁X₁ + β₂X₂ + … + βₚXₚ\n\\]\nOur goal is to estimate the coefficients (β’s) using Ordinary Least Squares (OLS) — the method that minimizes prediction error."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#why-linear-regression",
    "href": "weekly-notes/week-05-notes.html#why-linear-regression",
    "title": "Week 5 Notes",
    "section": "Why Linear Regression?",
    "text": "Why Linear Regression?\nAdvantages - Simple and interpretable\n- Performs surprisingly well for many real-world problems\n- Foundation for more advanced methods\nLimitations - Assumes linearity and independence\n- Sensitive to outliers\n- Requires diagnostic checks"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#prediction-vs-inference",
    "href": "weekly-notes/week-05-notes.html#prediction-vs-inference",
    "title": "Week 5 Notes",
    "section": "Prediction vs Inference",
    "text": "Prediction vs Inference\nInference asks:\n&gt; Does education significantly affect income?\nFocus: understanding mechanisms, testing hypotheses.\nPrediction asks:\n&gt; What will the income be for this county?\nFocus: accuracy and reliability of forecasts.\nThis session emphasizes prediction, while recognizing inference as a complementary goal."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#example-applications",
    "href": "weekly-notes/week-05-notes.html#example-applications",
    "title": "Week 5 Notes",
    "section": "Example Applications",
    "text": "Example Applications\n\nPrediction\nUsed by governments to: - Estimate income for areas with incomplete census data\n- Forecast population growth or public service needs\nAccurate predictions improve decision-making even without full causal understanding.\n\n\nInference\nUsed by researchers to: - Understand gentrification and inequality\n- Identify which neighborhood characteristics explain income changes\n- Evaluate the impact of education or policy interventions"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connection-to-week-2-algorithmic-bias",
    "href": "weekly-notes/week-05-notes.html#connection-to-week-2-algorithmic-bias",
    "title": "Week 5 Notes",
    "section": "Connection to Week 2: Algorithmic Bias",
    "text": "Connection to Week 2: Algorithmic Bias\nStatistical accuracy does not guarantee ethical fairness.\nFor example, a healthcare model once predicted medical needs using costs as a proxy — resulting in racial bias.\nThis highlights that fit metrics (like R²) cannot replace ethical evaluation."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#interpreting-regression-results",
    "href": "weekly-notes/week-05-notes.html#interpreting-regression-results",
    "title": "Week 5 Notes",
    "section": "Interpreting Regression Results",
    "text": "Interpreting Regression Results\nWhen fitting a simple model of median income vs. population: - The intercept (~$62,855) represents baseline income when population = 0 (not practically meaningful).\n- The slope (~$0.02 per person) means that, on average, income rises $20 for every additional 1,000 people.\n- The relationship is statistically significant (p &lt; 0.001).\n- R² = 0.21 → about 21% of variation in income is explained by population.\nThis shows population helps explain income differences, though other variables matter too."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#model-evaluation-key-ideas",
    "href": "weekly-notes/week-05-notes.html#model-evaluation-key-ideas",
    "title": "Week 5 Notes",
    "section": "Model Evaluation: Key Ideas",
    "text": "Model Evaluation: Key Ideas\n\nThe “True” vs. Estimated Relationship\nThe real relationship (f) is unobservable.\nOur model provides an approximation based on available data.\nEach dataset produces slightly different estimates — uncertainty is measured by standard errors.\n\n\nStatistical Significance\n\nNull hypothesis (H₀): β₁ = 0 (no relationship)\n\nA small p-value means the observed effect is unlikely to be random → the relationship is real.\n\n\n\nR² and Model Fit\n\nR² measures the share of variance in Y explained by X.\n\nIt indicates strength, not correctness, of the model.\n\nA model with low R² may still provide useful predictions."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#overfitting-and-model-validation",
    "href": "weekly-notes/week-05-notes.html#overfitting-and-model-validation",
    "title": "Week 5 Notes",
    "section": "Overfitting and Model Validation",
    "text": "Overfitting and Model Validation\nA model can perform well on training data but fail on new data.\nTo avoid this, data are split into training and testing sets.\n\nTraining error measures fit on known data.\n\nTesting error measures predictive performance on unseen data.\n\nGood models balance bias and variance, avoiding both underfitting and overfitting."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#model-assumptions-and-diagnostics",
    "href": "weekly-notes/week-05-notes.html#model-assumptions-and-diagnostics",
    "title": "Week 5 Notes",
    "section": "Model Assumptions and Diagnostics",
    "text": "Model Assumptions and Diagnostics\n\nLinearity — relationship between variables should be roughly linear.\n\nCheck with residual plots.\n\nCurvature suggests model misspecification.\n\nConstant Variance (Homoscedasticity) — residual spread should be even.\n\nViolations make p-values unreliable.\n\nSolutions: transform Y, add predictors, or use robust errors.\n\nNormality of Residuals — important for hypothesis testing.\n\nUse Q–Q plots to assess normal distribution.\n\nNo Multicollinearity — predictors should not be highly correlated.\n\nHigh correlation makes coefficients unstable.\n\nNo Influential Outliers — avoid points that disproportionately shape the regression line.\n\nDetect using Cook’s Distance."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#improving-the-model",
    "href": "weekly-notes/week-05-notes.html#improving-the-model",
    "title": "Week 5 Notes",
    "section": "Improving the Model",
    "text": "Improving the Model\n\nAdding Predictors\nIncluding education and poverty rates typically improves explanatory power.\nExample: Adjusted R² increases to around 0.57, suggesting stronger fit.\n\n\nTransformations\nIf relationships are curved, use log transformations.\nFor instance, a log–population model may capture diminishing returns to scale.\n\n\nCategorical Variables\nInclude binary variables such as metro vs. non-metro areas.\nIn one model, metro counties earned about $30,000 more on average."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#summary-the-regression-workflow",
    "href": "weekly-notes/week-05-notes.html#summary-the-regression-workflow",
    "title": "Week 5 Notes",
    "section": "Summary: The Regression Workflow",
    "text": "Summary: The Regression Workflow\n\nUnderstand the conceptual model (f(X)).\n\nVisualize relationships before modeling.\n\nFit the regression model carefully.\n\nEvaluate performance using validation methods.\n\nCheck key assumptions.\n\nRefine with new variables or transformations.\n\nReflect on the ethical and social implications of modeling."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-takeaways",
    "href": "weekly-notes/week-05-notes.html#key-takeaways",
    "title": "Week 5 Notes",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nStatistical learning is about estimating f(X), the systematic link between predictors and outcomes.\n\nTwo central goals:\n\nInference – understand relationships\n\nPrediction – forecast new outcomes\n\n\nGood fit ≠ good model: always assess assumptions and fairness.\n\nDiagnostics matter: plots and reasoning reveal what statistics can hide.\n\nEthical awareness is essential in all modeling decisions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nMy name is Xiao Yu, you can call me Cathy. I am a MUSA student. I completed my undergraduate degree in Urban Studies and Planning at the University of Sheffield.\n\n\n\n\n\nEmail: [uxiaoo22@upenn.edu]\nGitHub: [@uxiaoo22]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "My name is Xiao Yu, you can call me Cathy. I am a MUSA student. I completed my undergraduate degree in Urban Studies and Planning at the University of Sheffield."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [uxiaoo22@upenn.edu]\nGitHub: [@uxiaoo22]"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html",
    "href": "Assignments/Assignment_1/Assignment_1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Texas Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#scenario",
    "href": "Assignments/Assignment_1/Assignment_1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Texas Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#learning-objectives",
    "href": "Assignments/Assignment_1/Assignment_1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#submission-instructions",
    "href": "Assignments/Assignment_1/Assignment_1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#data-retrieval",
    "href": "Assignments/Assignment_1/Assignment_1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_data_2022 &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",       # Total population\n    med_household_income = \"B19013_001\"   # Median household income\n    ),\n  state = \"TX\",\n  year = 2022,\n  output = \"wide\"\n)\n# Clean the county names to remove state name and \"County\" \ncounty_data_2022 &lt;- county_data_2022 %&gt;%\n  mutate(county_name = str_remove(NAME, \", Texas\"))\n\n# Display the first few rows\nhead(county_data_2022)\n\n# A tibble: 6 × 7\n  GEOID NAME   total_popE total_popM med_household_incomeE med_household_incomeM\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;                 &lt;dbl&gt;                 &lt;dbl&gt;\n1 48001 Ander…      58077         NA                 57445                  4562\n2 48003 Andre…      18362         NA                 86458                 16116\n3 48005 Angel…      86608         NA                 57055                  2484\n4 48007 Arans…      24048         NA                 58168                  6458\n5 48009 Arche…       8649         NA                 69954                  8482\n6 48011 Armst…       1912        145                 70417                 14574\n# ℹ 1 more variable: county_name &lt;chr&gt;"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#data-quality-assessment",
    "href": "Assignments/Assignment_1/Assignment_1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\nlibrary(scales)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Calculate MOE percentage and reliability categories using mutate()\nTX_county_reliability &lt;- county_data_2022 %&gt;%\n  mutate(\n    med_income_moe_pct = (med_household_incomeM / med_household_incomeE) * 100,\n    med_income_confi = case_when(\n      med_income_moe_pct &lt; 5 ~ \"High Confidence (&lt;5%)\",\n      med_income_moe_pct &gt; 5 & med_income_moe_pct &lt;10 ~ \"Moderate Confidence (5% - 10%)\",\n      med_income_moe_pct &gt; 10  ~ \"Low Confidence (&gt;10%)\"\n ),\n    unreliable_income = med_income_moe_pct &gt;= 10\n  )\n# Create a summary showing count of counties in each reliability category\nTX_reliability_summary &lt;- TX_county_reliability %&gt;%\n  count(med_income_confi) %&gt;%\n  mutate(percent = round(100 * n / sum(n), 1))\n\n# Display the summary table\nkable(\n  TX_reliability_summary,\n  caption = \"Texas county-level median household income reliability (ACS 2022)\",\n  col.names = c(\"Reliability Category\", \"Count\", \"Percentage\"),\n  align = c(\"l\", \"r\", \"r\")\n) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\n\nTexas county-level median household income reliability (ACS 2022)\n\n\nReliability Category\nCount\nPercentage\n\n\n\n\nHigh Confidence (&lt;5%)\n58\n22.8\n\n\nLow Confidence (&gt;10%)\n113\n44.5\n\n\nModerate Confidence (5% - 10%)\n82\n32.3\n\n\nNA\n1\n0.4\n\n\n\n\n\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#high-uncertainty-counties",
    "href": "Assignments/Assignment_1/Assignment_1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\nlibrary(scales)\nlibrary(kableExtra)\n\n# Create table of top 5 counties by MOE percentage\nTX_high_uncertainty &lt;- TX_county_reliability %&gt;%\n  arrange(desc(med_income_moe_pct)) %&gt;%\n  slice(1:5) %&gt;%\n  select(\n    County = county_name,\n    `Median Income ($)` = med_household_incomeE,\n    `Margin of Error ($)` = med_household_incomeM,\n    `MOE (%)` = med_income_moe_pct,\n    Reliability = med_income_confi\n  ) %&gt;%\n  # Format numbers for professional output\n  mutate(\n    `Median Income ($)` = dollar(`Median Income ($)`),\n    `Margin of Error ($)` = dollar(`Margin of Error ($)`),\n    `MOE (%)` = paste0(round(`MOE (%)`, 1), \"%\")\n  )\n\n# Format as table with kable() - include appropriate column names and caption\nkable(\n  TX_high_uncertainty,\n  caption = \"Top 5 Texas Counties by Margin of Error in Median Household Income (ACS 2022)\",\n  align = c(\"l\", \"r\", \"r\", \"r\", \"l\")  # set column alignment\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\")\n\n\nTop 5 Texas Counties by Margin of Error in Median Household Income (ACS 2022)\n\n\nCounty\nMedian Income ($)\nMargin of Error ($)\nMOE (%)\nReliability\n\n\n\n\nJeff Davis County\n$38,125\n$25,205\n66.1%\nLow Confidence (&gt;10%)\n\n\nCulberson County\n$35,924\n$18,455\n51.4%\nLow Confidence (&gt;10%)\n\n\nKing County\n$59,375\n$29,395\n49.5%\nLow Confidence (&gt;10%)\n\n\nKinney County\n$52,386\n$23,728\n45.3%\nLow Confidence (&gt;10%)\n\n\nDimmit County\n$27,374\n$12,374\n45.2%\nLow Confidence (&gt;10%)\n\n\n\n\n\nData Quality Commentary:\nThe five Texas counties with the highest margins of error in median household income estimates—Jeff Davis, Culberson, King, Kinney, and Dimmit—show MOE percentages ranging from 45% to 66%. Such extreme levels indicate that ACS estimates for these areas are highly unreliable. The primary causes are small populations, limited survey samples, and income variability that magnifies error. If used directly in algorithmic decision-making, these data could misclassify community needs and distort funding priorities. Policymakers should instead supplement ACS data with administrative or tax records, or require manual review, to ensure fair and accurate resource allocation."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#focus-area-selection",
    "href": "Assignments/Assignment_1/Assignment_1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\nlibrary(scales)\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_counties &lt;- TX_county_reliability %&gt;%\n  filter(med_household_incomeE %in% c(35924, 27374)) %&gt;%\n  mutate(`MOE (%)` = round(med_income_moe_pct, 1)) %&gt;%\n  select(\n    County = county_name,\n    `Median Income ($)` = med_household_incomeE,\n    `MOE (%)`,\n    Reliability = med_income_confi\n  )\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n# Display the selected counties\nkable(\n  selected_counties,\n  caption = \"Selected Texas Counties for Tract-Level Analysis\",\n  align = c(\"l\", \"r\", \"r\", \"r\", \"l\")\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\", bootstrap_options = c(\"striped\", \"hover\"))\n\n\nSelected Texas Counties for Tract-Level Analysis\n\n\nCounty\nMedian Income ($)\nMOE (%)\nReliability\n\n\n\n\nCulberson County\n35924\n51.4\nLow Confidence (&gt;10%)\n\n\nDimmit County\n27374\n45.2\nLow Confidence (&gt;10%)\n\n\n\n\n\nComment on the output:Culberson and Dimmit Counties were selected as examples of Low Confidence data. Their high MOE values reflect the challenges of using ACS estimates in small, rural counties."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#tract-level-demographics",
    "href": "Assignments/Assignment_1/Assignment_1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nrace_vars &lt;- get_acs(\n  geography = \"tract\",\n  survey = \"acs5\",\n  variables = c(\n    white = \"B03002_003\", \n    black = \"B03002_004\",\n    hisp_latinx = \"B03002_012\",\n    total_pop = \"B03002_001\"\n  ),\n  year = 2022,\n  state = \"TX\",\n  county = c(\"109\", \"127\"),  # Culberson = 48109, Dimmit = 48127\n  output = \"wide\"\n)\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\nrace_vars &lt;- race_vars %&gt;%\n  mutate(\n    pct_white = 100 * whiteE / total_popE,\n    pct_black = 100 * blackE / total_popE,\n    pct_hispanic = 100 * hisp_latinxE / total_popE,\n    \n    # Split NAME on semicolon to extract tract and county\n    tract_name = sapply(strsplit(NAME, \";\"), function(x) trimws(x[1])),\n    county_name = sapply(strsplit(NAME, \";\"), function(x) trimws(x[2])) %&gt;%\n                  str_remove(\" County\")\n  )\n# Inspect first few rows\nhead(race_vars %&gt;%\n       select(tract_name, county_name, pct_white, pct_black, pct_hispanic))\n\n# A tibble: 4 × 5\n  tract_name           county_name pct_white pct_black pct_hispanic\n  &lt;chr&gt;                &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 Census Tract 9503    Culberson       11.4      0.183         81.3\n2 Census Tract 9502.01 Dimmit           7.39     0.242         91.7\n3 Census Tract 9502.02 Dimmit          17.2      1.56          77.2\n4 Census Tract 9504    Dimmit           4.20     0.156         93.8\n\n# Add readable tract and county name columns using str_extract() or similar"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#demographic-analysis",
    "href": "Assignments/Assignment_1/Assignment_1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\nlibrary(dplyr)\nlibrary(knitr)\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\ntop_hispanic_tract &lt;- race_vars %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  transmute(\n    Tract = tract_name,\n    County = county_name,\n    `Hispanic %` = percent(pct_hispanic / 100, accuracy = 0.1)\n  )\n\nkable(top_hispanic_tract,\n      caption = \"Tract with Highest Hispanic/Latino Population\",\n      align = c(\"l\", \"l\", \"r\")) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\",\n                bootstrap_options = c(\"striped\", \"hover\"))\n\n\nTract with Highest Hispanic/Latino Population\n\n\nTract\nCounty\nHispanic %\n\n\n\n\nCensus Tract 9504\nDimmit\n93.8%\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\ncounty_summary &lt;- race_vars %&gt;%\n  group_by(county_name) %&gt;%\n  summarise(\n    `Number of Tracts` = n(),\n    `Avg. White %` = percent(mean(pct_white, na.rm = TRUE) / 100, accuracy = 0.1),\n    `Avg. Black %` = percent(mean(pct_black, na.rm = TRUE) / 100, accuracy = 0.1),\n    `Avg. Hispanic %` = percent(mean(pct_hispanic, na.rm = TRUE) / 100, accuracy = 0.1)\n  )\n# Create a nicely formatted table of your results using kable()\nkable(\n  county_summary,\n  caption = \"Average Demographic Composition by County\",\n  align = c(\"l\", \"r\", \"r\", \"r\", \"r\")\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\",\n                bootstrap_options = c(\"striped\", \"hover\"))\n\n\nAverage Demographic Composition by County\n\n\ncounty_name\nNumber of Tracts\nAvg. White %\nAvg. Black %\nAvg. Hispanic %\n\n\n\n\nCulberson\n1\n11.4%\n0.2%\n81.3%\n\n\nDimmit\n3\n9.6%\n0.7%\n87.6%"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#moe-analysis-for-demographic-variables",
    "href": "Assignments/Assignment_1/Assignment_1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\ndemo_moe &lt;- race_vars %&gt;%\n  mutate(\n    white_moe_pct = (whiteM / whiteE) * 100,\n    black_moe_pct = (blackM / blackE) * 100,\n    hispanic_moe_pct = (hisp_latinxM / hisp_latinxE) * 100,\n    \n    # Flag tracts where any demographic MOE &gt; 15%\n    high_moe_flag = ifelse(\n      white_moe_pct &gt; 15 | black_moe_pct &gt; 15 | hispanic_moe_pct &gt; 15,\n      TRUE, FALSE\n    )\n  )\n# Create summary statistics showing how many tracts have data quality issues\nmoe_summary_county &lt;- demo_moe %&gt;%\n  group_by(county_name) %&gt;%\n  summarise(\n    total_tracts = n(),\n    high_moe_tracts = sum(high_moe_flag, na.rm = TRUE),\n    percent_high_moe = round(100 * mean(high_moe_flag, na.rm = TRUE), 1)\n  )\n\nkable(\n  moe_summary_county,\n  caption = \"**MOE Summary by County (ACS 2022)**\",\n  row.names = FALSE,\n  col.names = c(\"County\", \"Total Tracts\", \"High MOE Tracts\", \"Percent High MOE (%)\"),\n  align = c(\"l\", \"c\", \"c\", \"r\")\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\",\n                bootstrap_options = c(\"striped\", \"hover\"))\n\n\n**MOE Summary by County (ACS 2022)**\n\n\nCounty\nTotal Tracts\nHigh MOE Tracts\nPercent High MOE (%)\n\n\n\n\nCulberson\n1\n1\n100\n\n\nDimmit\n3\n3\n100"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#pattern-analysis",
    "href": "Assignments/Assignment_1/Assignment_1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Group tracts by whether they have high MOE issues\nmoe_patterns &lt;- demo_moe %&gt;%\n  filter(high_moe_flag == TRUE) %&gt;%   # keep only high-MOE tracts\n  group_by(county_name) %&gt;%\n  summarise(\n    `Population Average` = round(mean(total_popE, na.rm = TRUE), 0),\n    `% White Avg` = round(mean(pct_white, na.rm = TRUE), 2),\n    `% Black Avg` = round(mean(pct_black, na.rm = TRUE), 2),\n    `% LatinX Avg` = round(mean(pct_hispanic, na.rm = TRUE), 2),\n    `Tracts Quantity` = n(),\n    .groups = \"drop\"\n  )\n\nkable(\n  moe_patterns,\n  caption = \"**High-MOE Tracts by County (ACS 2022)**\",\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"r\"),\n  col.names = c(\"County\", \"Population Average\", \"% White Avg\", \"% Black Avg\", \"% LatinX Avg\", \"Tracts Quantity\"),\n  digits = 2\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\",\n                bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n**High-MOE Tracts by County (ACS 2022)**\n\n\nCounty\nPopulation Average\n% White Avg\n% Black Avg\n% LatinX Avg\nTracts Quantity\n\n\n\n\nCulberson\n2181\n11.42\n0.18\n81.34\n1\n\n\nDimmit\n2891\n9.60\n0.65\n87.57\n3\n\n\n\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nPattern Analysis: High MOE tracts are found in small, rural counties with low populations and high Hispanic/Latino shares.These factors reduce ACS reliability, meaning minority and rural communities face greater risk of being misrepresented in algorithmic decisions."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#analysis-integration-and-professional-summary",
    "href": "Assignments/Assignment_1/Assignment_1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\nLooking at ACS 2022 data for Texas, we see that data quality isn’t the same everywhere. Larger, urban counties usually have solid numbers, while smaller rural counties often have very high margins of error. In some places, the income data is so uncertain that it’s hard to use with confidence.\nThe biggest risk shows up in rural, Hispanic-majority counties like Dimmit and Culberson. These areas often have the highest uncertainty, which means if an algorithm used this data directly, the results could shortchange the very communities that need the most support.\nThe main reason for this problem is how the ACS survey works. Small populations naturally create bigger sampling errors, and rural or minority communities may also face challenges like language barriers or low response rates. That makes their numbers less reliable.\nTo make decisions fairer, a tiered approach makes sense. Counties with strong data can go into the algorithm as-is. Counties with moderate-quality data should be monitored, and counties with weak data should get extra checks or even manual review. Over time, improving census participation in rural and minority communities would help fix the root of the problem."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#specific-recommendations",
    "href": "Assignments/Assignment_1/Assignment_1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\nlibrary(scales)\nlibrary(kableExtra)\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\nTX_recommendations &lt;- TX_county_reliability %&gt;%\n  select(county_name, med_household_incomeE, med_income_moe_pct, med_income_confi) %&gt;%\n  mutate(\n    `Median Income ($)` = dollar(med_household_incomeE),\n    `MOE (%)` = percent(med_income_moe_pct / 100, accuracy = 0.1),\n    Recommendation = case_when(\n      med_income_confi == \"High Confidence (&lt;5%)\" ~ \"✅ Safe for algorithmic decisions\",\n      med_income_confi == \"Moderate Confidence (5% - 10%)\" ~ \"⚠️ Use with caution – monitor outcomes\",\n      med_income_confi == \"Low Confidence (&gt;10%)\" ~ \"❌ Requires manual review or extra data\",\n      TRUE ~ \"Check data\"\n    )\n  ) %&gt;%\n  select(county_name, `Median Income ($)`, `MOE (%)`, med_income_confi, Recommendation)\n\n\n# Format as a professional table with kable()\nkable(\n  TX_recommendations,\n  caption = \"**County-Level Reliability and Algorithm Recommendations (ACS 2022)**\",\n  col.names = c(\"County\", \"Median Income ($)\", \"MOE (%)\", \"Reliability\", \"Recommendation\"),\n  escape = FALSE,\n  align = c(\"l\", \"r\", \"r\", \"l\", \"l\")\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\",\n                bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) %&gt;%\n  column_spec(1, bold = TRUE)\n\n\n**County-Level Reliability and Algorithm Recommendations (ACS 2022)**\n\n\nCounty\nMedian Income ($)\nMOE (%)\nReliability\nRecommendation\n\n\n\n\nAnderson County\n$57,445\n7.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nAndrews County\n$86,458\n18.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nAngelina County\n$57,055\n4.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nAransas County\n$58,168\n11.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nArcher County\n$69,954\n12.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nArmstrong County\n$70,417\n20.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nAtascosa County\n$67,442\n6.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nAustin County\n$73,556\n6.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBailey County\n$69,830\n18.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBandera County\n$70,965\n8.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBastrop County\n$80,151\n6.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBaylor County\n$52,716\n25.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBee County\n$50,283\n10.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBell County\n$62,858\n2.8%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBexar County\n$67,275\n1.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBlanco County\n$79,717\n9.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBorden County\n$80,625\n24.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBosque County\n$63,868\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBowie County\n$56,628\n4.1%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBrazoria County\n$91,972\n3.3%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBrazos County\n$57,562\n3.5%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBrewster County\n$47,747\n11.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBriscoe County\n$35,446\n27.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBrooks County\n$30,566\n33.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nBrown County\n$53,792\n4.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nBurleson County\n$71,745\n6.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nBurnet County\n$71,482\n8.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCaldwell County\n$66,779\n7.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCalhoun County\n$62,267\n9.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCallahan County\n$63,906\n3.6%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nCameron County\n$47,435\n3.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nCamp County\n$53,968\n7.6%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCarson County\n$83,199\n4.5%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nCass County\n$54,303\n6.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCastro County\n$59,886\n17.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nChambers County\n$106,103\n8.3%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCherokee County\n$56,971\n8.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nChildress County\n$56,063\n29.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nClay County\n$75,227\n7.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCochran County\n$41,597\n17.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCoke County\n$40,230\n13.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nColeman County\n$51,034\n7.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCollin County\n$113,255\n1.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nCollingsworth County\n$52,045\n22.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nColorado County\n$63,352\n8.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nComal County\n$93,744\n2.8%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nComanche County\n$57,383\n13.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nConcho County\n$55,750\n27.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCooke County\n$66,374\n8.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nCoryell County\n$63,281\n3.6%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nCottle County\n$47,625\n37.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCrane County\n$71,364\n32.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCrockett County\n$64,103\n34.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCrosby County\n$50,268\n10.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nCulberson County\n$35,924\n51.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDallam County\n$71,969\n9.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nDallas County\n$70,732\n0.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nDawson County\n$45,268\n27.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDeaf Smith County\n$51,942\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nDelta County\n$68,491\n27.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDenton County\n$104,180\n1.3%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nDeWitt County\n$61,100\n7.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nDickens County\n$46,638\n13.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDimmit County\n$27,374\n45.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDonley County\n$51,711\n12.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nDuval County\n$50,697\n20.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nEastland County\n$52,902\n12.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nEctor County\n$70,566\n4.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nEdwards County\n$40,809\n27.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nEllis County\n$93,248\n2.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nEl Paso County\n$55,417\n1.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nErath County\n$59,654\n6.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nFalls County\n$45,172\n15.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nFannin County\n$65,835\n6.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nFayette County\n$72,881\n5.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nFisher County\n$60,461\n8.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nFloyd County\n$49,321\n9.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nFoard County\n$41,944\n20.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nFort Bend County\n$109,987\n2.6%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nFranklin County\n$67,915\n4.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nFreestone County\n$55,902\n10.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nFrio County\n$56,042\n30.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nGaines County\n$73,299\n13.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nGalveston County\n$83,913\n2.8%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nGarza County\n$56,215\n35.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nGillespie County\n$70,162\n8.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nGlasscock County\n$112,188\n27.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nGoliad County\n$58,125\n25.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nGonzales County\n$64,255\n8.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nGray County\n$54,563\n7.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nGrayson County\n$66,608\n3.6%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nGregg County\n$63,811\n3.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nGrimes County\n$63,484\n9.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nGuadalupe County\n$88,111\n3.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHale County\n$50,721\n9.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHall County\n$43,873\n11.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHamilton County\n$54,890\n17.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHansford County\n$62,350\n19.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHardeman County\n$60,455\n15.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHardin County\n$70,164\n5.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHarris County\n$70,789\n0.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHarrison County\n$63,427\n4.8%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHartley County\n$78,065\n27.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHaskell County\n$52,786\n16.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHays County\n$79,990\n3.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHemphill County\n$67,798\n27.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHenderson County\n$59,778\n4.3%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHidalgo County\n$49,371\n2.3%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHill County\n$60,669\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHockley County\n$53,283\n7.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHood County\n$80,013\n4.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHopkins County\n$63,766\n5.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHouston County\n$51,043\n10.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHoward County\n$67,243\n6.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nHudspeth County\n$35,163\n23.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nHunt County\n$66,885\n4.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nHutchinson County\n$62,211\n7.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nIrion County\n$54,708\n17.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJack County\n$58,861\n13.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJackson County\n$67,176\n17.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJasper County\n$48,818\n9.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nJeff Davis County\n$38,125\n66.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJefferson County\n$57,294\n2.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nJim Hogg County\n$42,292\n13.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJim Wells County\n$46,626\n12.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nJohnson County\n$77,058\n3.1%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nJones County\n$59,361\n10.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKarnes County\n$57,798\n14.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKaufman County\n$84,075\n4.1%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nKendall County\n$104,196\n8.3%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nKenedy County\n$45,455\n25.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKent County\n$68,553\n15.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKerr County\n$66,713\n6.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nKimble County\n$62,386\n22.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKing County\n$59,375\n49.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKinney County\n$52,386\n45.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nKleberg County\n$52,487\n9.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nKnox County\n$48,750\n9.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLamar County\n$58,246\n4.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nLamb County\n$54,519\n8.6%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLampasas County\n$73,269\n7.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLa Salle County\n$62,798\n26.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nLavaca County\n$58,530\n7.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLee County\n$66,448\n10.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nLeon County\n$57,363\n12.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nLiberty County\n$59,605\n6.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLimestone County\n$53,102\n7.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLipscomb County\n$71,625\n12.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nLive Oak County\n$55,949\n18.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nLlano County\n$64,241\n8.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nLoving County\nNA\nNA\nNA\nCheck data\n\n\nLubbock County\n$61,911\n3.5%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nLynn County\n$52,996\n7.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMcCulloch County\n$53,214\n16.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMcLennan County\n$59,781\n3.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nMcMullen County\n$60,313\n41.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMadison County\n$65,768\n9.6%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMarion County\n$48,040\n5.0%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nMartin County\n$70,217\n27.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMason County\n$77,583\n15.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMatagorda County\n$56,412\n6.3%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMaverick County\n$48,497\n10.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMedina County\n$73,060\n4.0%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nMenard County\n$40,945\n17.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMidland County\n$90,123\n5.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMilam County\n$56,985\n5.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMills County\n$59,315\n9.3%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMitchell County\n$49,869\n12.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nMontague County\n$63,336\n8.6%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMontgomery County\n$95,946\n3.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nMoore County\n$59,041\n6.5%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMorris County\n$51,532\n6.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nMotley County\n$66,528\n8.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nNacogdoches County\n$51,153\n4.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nNavarro County\n$56,261\n7.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nNewton County\n$38,871\n16.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nNolan County\n$47,437\n7.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nNueces County\n$64,027\n2.3%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nOchiltree County\n$62,240\n17.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nOldham County\n$71,103\n11.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nOrange County\n$71,910\n7.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nPalo Pinto County\n$65,242\n4.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nPanola County\n$58,205\n18.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nParker County\n$95,721\n3.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nParmer County\n$65,575\n13.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nPecos County\n$59,325\n17.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nPolk County\n$57,315\n5.2%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nPotter County\n$47,974\n4.0%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nPresidio County\n$29,012\n24.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRains County\n$60,291\n10.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRandall County\n$78,038\n3.5%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nReagan County\n$70,319\n12.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nReal County\n$46,842\n33.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRed River County\n$44,583\n9.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nReeves County\n$57,487\n22.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRefugio County\n$54,304\n4.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nRoberts County\n$62,667\n14.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRobertson County\n$59,410\n17.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nRockwall County\n$121,303\n3.8%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nRunnels County\n$55,424\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nRusk County\n$61,661\n9.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nSabine County\n$47,061\n16.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSan Augustine County\n$45,888\n9.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nSan Jacinto County\n$54,839\n13.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSan Patricio County\n$63,842\n6.9%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nSan Saba County\n$54,087\n16.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSchleicher County\n$53,774\n15.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nScurry County\n$58,932\n21.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nShackelford County\n$60,924\n14.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nShelby County\n$49,231\n10.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nSherman County\n$66,169\n27.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSmith County\n$69,053\n3.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nSomervell County\n$87,899\n33.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nStarr County\n$35,979\n8.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nStephens County\n$44,712\n18.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSterling County\n$63,558\n22.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nStonewall County\n$66,591\n32.5%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSutton County\n$56,778\n22.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nSwisher County\n$40,290\n13.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nTarrant County\n$78,872\n1.0%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nTaylor County\n$61,806\n3.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nTerrell County\n$52,813\n21.0%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nTerry County\n$42,694\n10.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nThrockmorton County\n$55,221\n21.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nTitus County\n$57,634\n8.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nTom Green County\n$67,215\n4.7%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nTravis County\n$92,731\n1.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nTrinity County\n$51,165\n11.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nTyler County\n$50,898\n10.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nUpshur County\n$60,456\n7.7%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nUpton County\n$55,284\n21.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nUvalde County\n$55,000\n15.3%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nVal Verde County\n$57,250\n8.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nVan Zandt County\n$62,334\n8.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nVictoria County\n$66,308\n3.6%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nWalker County\n$47,193\n6.3%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nWaller County\n$71,643\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nWard County\n$70,771\n12.6%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nWashington County\n$70,043\n9.4%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nWebb County\n$59,984\n3.1%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nWharton County\n$59,712\n6.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nWheeler County\n$58,158\n14.2%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nWichita County\n$58,862\n3.2%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nWilbarger County\n$50,769\n18.4%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nWillacy County\n$42,839\n13.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nWilliamson County\n$102,851\n1.4%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nWilson County\n$89,708\n4.9%\nHigh Confidence (&lt;5%)\n✅ Safe for algorithmic decisions |\n\n\nWinkler County\n$89,155\n16.1%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nWise County\n$85,385\n6.1%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nWood County\n$61,748\n6.0%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nYoakum County\n$80,317\n8.8%\nModerate Confidence (5% - 10%)\n⚠️ Use with caution – monitor outcomes\n\n\nYoung County\n$65,565\n16.9%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nZapata County\n$35,061\n10.7%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\nZavala County\n$49,243\n28.8%\nLow Confidence (&gt;10%)\n❌ Requires manual review or extra data |\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: High-confidence counties such as Bexar, Dallas, Travis, Williamson, and Collin have strong data quality (MOE &lt;5%). These counties are large, urban, and well-sampled, making them reliable for algorithm-driven funding and planning decisions.\nCounties requiring additional oversight: Moderate-confidence counties like Anderson, Bastrop, Caldwell, and Wise fall in the 5–10% MOE range. Algorithms may be used here, but outcomes should be monitored regularly with human oversight to check for misallocations.\nCounties needing alternative approaches: Low-confidence counties such as Dimmit, Culberson, Jeff Davis, King, and Kinney have very high MOEs (&gt;10%, sometimes above 40–60%). These areas need manual review, supplemental surveys, or local administrative data to ensure fair resource distribution."
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#questions-for-further-investigation",
    "href": "Assignments/Assignment_1/Assignment_1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nAre high-MOE counties geographically clustered (e.g., along the border or in rural west Texas), and does this spatial pattern affect equity?\nHow do data quality issues change over time — are counties improving or declining in reliability across ACS cycles?\nDo certain demographic groups (Hispanic, Black, rural populations) consistently face higher MOEs, and what targeted outreach could improve data collection?"
  },
  {
    "objectID": "Assignments/Assignment_1/Assignment_1.html#submission-checklist",
    "href": "Assignments/Assignment_1/Assignment_1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\n[☑ ] All code chunks run without errors\n[☑ ] All “[Fill this in]” prompts have been completed\n[☑ ] Tables are properly formatted and readable\n[☑ ] Executive summary addresses all four required components\n[☑ ] Portfolio navigation includes this assignment\n[☑ ] Census API key is properly set\n[☑ ] Document renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(\"ec702835845a134b4376c60759aa72ce62f6df59\")\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#setup-and-data-loading",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#setup-and-data-loading",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(\"ec702835845a134b4376c60759aa72ce62f6df59\")\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 0: Finding Census Variable Codes",
    "text": "Exercise 0: Finding Census Variable Codes\nThe Challenge: You know you want data on total population, median income, and median age, but you don’t know the specific Census variable codes. How do you find them?\n\n0.1 Load the Variable Dictionary\n\n# Load all available variables for ACS 5-year 2022\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# Look at the structure\nglimpse(acs_vars_2022)\n\nRows: 28,152\nColumns: 4\n$ name      &lt;chr&gt; \"B01001A_001\", \"B01001A_002\", \"B01001A_003\", \"B01001A_004\", …\n$ label     &lt;chr&gt; \"Estimate!!Total:\", \"Estimate!!Total:!!Male:\", \"Estimate!!To…\n$ concept   &lt;chr&gt; \"Sex by Age (White Alone)\", \"Sex by Age (White Alone)\", \"Sex…\n$ geography &lt;chr&gt; \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract…\n\nhead(acs_vars_2022)\n\n# A tibble: 6 × 4\n  name        label                                   concept          geography\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;            &lt;chr&gt;    \n1 B01001A_001 Estimate!!Total:                        Sex by Age (Whi… tract    \n2 B01001A_002 Estimate!!Total:!!Male:                 Sex by Age (Whi… tract    \n3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years  Sex by Age (Whi… tract    \n4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years   Sex by Age (Whi… tract    \n5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years Sex by Age (Whi… tract    \n6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years Sex by Age (Whi… tract    \n\n\nWhat you see:\n\nname: The variable code (e.g., “B01003_001”)\nlabel: Human-readable description\nconcept: The broader table this variable belongs to\n\n\n\n0.2 Search for Population Variables\nYour Task: Find the variable code for total population.\n\n# Search for population-related variables\npopulation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"Total.*population\"))\n\n# Look at the results\nhead(population_vars, 10)\n\n# A tibble: 10 × 4\n   name       label                                            concept geography\n   &lt;chr&gt;      &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n 1 B16008_002 \"Estimate!!Total:!!Native population:\"           Citize… tract    \n 2 B16008_003 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 3 B16008_004 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 4 B16008_005 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 5 B16008_006 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 6 B16008_007 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 7 B16008_008 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 8 B16008_009 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 9 B16008_010 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n10 B16008_011 \"Estimate!!Total:!!Native population:!!18 years… Citize… tract    \n\n# Or search in the concept field\npop_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Total Population\"))\n\nhead(pop_concept)\n\n# A tibble: 6 × 4\n  name        label                             concept                geography\n  &lt;chr&gt;       &lt;chr&gt;                             &lt;chr&gt;                  &lt;chr&gt;    \n1 B01003_001  Estimate!!Total                   Total Population       block gr…\n2 B25008A_001 Estimate!!Total:                  Total Population in O… block gr…\n3 B25008A_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n4 B25008A_003 Estimate!!Total:!!Renter occupied Total Population in O… block gr…\n5 B25008B_001 Estimate!!Total:                  Total Population in O… block gr…\n6 B25008B_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n\n\nTip: Look for “Total” followed by “population” - usually B01003_001\n\n\n0.3 Search for Income Variables\nYour Task: Find median household income variables.\n\n# Search for median income\nincome_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\"))\n\n# Look specifically for household income\nhousehold_income &lt;- income_vars %&gt;%\n  filter(str_detect(label, \"household\"))\n\nprint(\"Household income variables:\")\n\n[1] \"Household income variables:\"\n\nhead(household_income)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B10010_002  Estimate!!Median family income in the past 12 m… Median… tract    \n2 B10010_003  Estimate!!Median family income in the past 12 m… Median… tract    \n3 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n6 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n\n# Alternative: search by concept\nincome_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Median Household Income\"))\n\nhead(income_concept)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n2 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n3 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013E_001 Estimate!!Median household income in the past 1… Median… county   \n6 B19013F_001 Estimate!!Median household income in the past 1… Median… tract    \n\n\nPattern Recognition: Median household income is typically B19013_001\n\n\n0.4 Search for Age Variables\nYour Task: Find median age variables."
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 1: Single Variable EDA",
    "text": "Exercise 1: Single Variable EDA\n\n1.1 Load and Inspect Data\n\n# Get county-level data for your state\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",       # Total population\n    median_income = \"B19013_001\",   # Median household income\n    median_age = \"B01002_001\"       # Median age\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(county_name = str_remove(NAME, paste0(\", Pennsylvania\")))\n\n# Basic inspection\nglimpse(county_data)\n\nRows: 67\nColumns: 9\n$ GEOID          &lt;chr&gt; \"42001\", \"42003\", \"42005\", \"42007\", \"42009\", \"42011\", \"…\n$ NAME           &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n$ total_popE     &lt;dbl&gt; 104604, 1245310, 65538, 167629, 47613, 428483, 122640, …\n$ total_popM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ median_incomeE &lt;dbl&gt; 78975, 72537, 61011, 67194, 58337, 74617, 59386, 60650,…\n$ median_incomeM &lt;dbl&gt; 3334, 869, 2202, 1531, 2606, 1191, 2058, 2167, 1516, 21…\n$ median_ageE    &lt;dbl&gt; 43.8, 40.6, 47.0, 44.9, 47.3, 39.9, 42.9, 43.9, 44.0, 4…\n$ median_ageM    &lt;dbl&gt; 0.2, 0.1, 0.2, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, …\n$ county_name    &lt;chr&gt; \"Adams County\", \"Allegheny County\", \"Armstrong County\",…\n\n\n\n\n1.2 Explore Income Distribution\nYour Task: Create a histogram of median household income and describe what you see.\n\n# Create histogram of median income\nggplot(county_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"lightpink\", alpha = 0.5) +\n  labs(\n    title = \"Distribution of Median Household Income\",\n    x = \"Median Household Income ($)\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n1.3 Box Plot for Outlier Detection\nYour Task: Create a boxplot to identify specific outlier counties.\n\n# Box plot to see outliers clearly\nggplot(county_data) +\n  aes(y = median_incomeE) +\n  geom_boxplot(fill = \"lightpink\", width = 0.5) +\n  labs(\n    title = \"Median Income Distribution with Outliers\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Identify the outlier counties\nincome_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(median_incomeE, 0.25, na.rm = TRUE),\n    Q3 = quantile(median_incomeE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = median_incomeE &lt; (Q1 - 1.5 * IQR) | median_incomeE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, median_incomeE)\n\nprint(\"Outlier counties:\")\n\n[1] \"Outlier counties:\"\n\nincome_outliers\n\n# A tibble: 3 × 2\n  county_name       median_incomeE\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 Bucks County              107826\n2 Chester County            118574\n3 Montgomery County         107441\n\n\n\n\n1.4 Challenge Exercise: Population Distribution\nYour Task: Create your own visualization of population distribution and identify outliers.\nRequirements:\n\nCreate a histogram of total population (total_popE)\nUse a different color than the income example (try “darkgreen” or “purple”)\nAdd appropriate labels and title\nCreate a boxplot to identify population outliers\nFind and list the 3 most populous and 3 least populous counties\n\n\n# Create histogram of total population\nggplot(county_data) +\n  aes(x = total_popE) +\n  geom_histogram(bins = 15, fill = \"lightpink\", alpha = 0.7) +\n  labs(\n    title = \"Total Population Distribution\",\n    x = \"Population\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels=comma)\n\n\n\n\n\n\n\n\n\n# Box plot to see outliers clearly\nggplot(county_data) +\n  aes(y = total_popE) +\n  geom_boxplot(fill = \"lightpink\", width = 0.5) +\n  labs(\n    title = \"Total population distribution with Outliers\",\n    y = \"Population\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels=comma)\n\n\n\n\n\n\n\n# Identify the outlier counties\npopulation_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(total_popE, 0.25, na.rm = TRUE),\n    Q3 = quantile(total_popE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = total_popE &lt; (Q1 - 1.5 * IQR) | total_popE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, total_popE)\n\nprint(\"Outlier counties:\")\n\n[1] \"Outlier counties:\"\n\npopulation_outliers\n\n# A tibble: 7 × 2\n  county_name         total_popE\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Allegheny County       1245310\n2 Bucks County            645163\n3 Chester County          536474\n4 Delaware County         575312\n5 Lancaster County        553202\n6 Montgomery County       856399\n7 Philadelphia County    1593208"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 2: Two Variable Relationships",
    "text": "Exercise 2: Two Variable Relationships\n\n2.1 Population vs Income Scatter Plot\nYour Task: Explore the relationship between population size and median income.\n\n# Basic scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point() +\n  labs(\n    title = \"Population vs Median Income\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n2.2 Add Trend Line and Labels\nYour Task: Improve the plot by adding a trend line and labeling interesting points.\n\n# Enhanced scatter plot with trend line\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"lightpink\") +\n  labs(\n    title = \"Population vs Median Income in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\ncorrelation &lt;- cor(county_data$total_popE, county_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Correlation coefficient:\", round(correlation, 3)))\n\n[1] \"Correlation coefficient: 0.457\"\n\n\n\n\n2.3 Deal with Skewed Data\nYour Task: The population data is highly skewed. Try a log transformation.\n\n# Log-transformed scatter plot\nggplot(county_data) +\n  aes(x = log(total_popE), y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Log(Population) vs Median Income\",\n    x = \"Log(Total Population)\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Does the log transformation reveal a clearer relationship? Yes. It’s a positive slope, it suggests that more populous counties tend to have higher median household incomes.\n\n\n2.4 Challenge Exercise: Age vs Income Relationship\nYour Task: Explore the relationship between median age and median income using different visualization techniques.\nRequirements:\n\nCreate a scatter plot with median age on x-axis and median income on y-axis\nUse red points (color = \"red\") with 50% transparency (alpha = 0.5)\nAdd a smooth trend line using method = \"loess\" instead of “lm”\nUse the “dark” theme (theme_dark())\nFormat the y-axis with dollar signs\nAdd a title that mentions both variables\n\n\n# Enhanced scatter plot with trend line\nggplot(county_data) +\n  aes(x = median_ageE, y = median_incomeE) +\n  geom_point(alpha = 0.5, color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Median Age vs Median Income in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Median Age\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau\"\n  ) +\n  theme_dark() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\ncorrelation &lt;- cor(county_data$total_popE, county_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Correlation coefficient:\", round(correlation, 3)))\n\n[1] \"Correlation coefficient: 0.457\""
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 3: Data Quality Visualization",
    "text": "Exercise 3: Data Quality Visualization\n\n3.1 Visualize Margins of Error\nYour Task: Create a visualization showing how data reliability varies across counties.\n\n# Calculate MOE percentages\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    income_moe_pct = (median_incomeM / median_incomeE) * 100,\n    pop_category = case_when(\n      total_popE &lt; 50000 ~ \"Small (&lt;50K)\",\n      total_popE &lt; 200000 ~ \"Medium (50K-200K)\",\n      TRUE ~ \"Large (200K+)\"\n    )\n  )\n\n# MOE by population size\nggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Data Reliability Decreases with Population Size\",\n    x = \"Total Population\",\n    y = \"Margin of Error (%)\",\n    caption = \"Red line = 10% reliability threshold\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n\n\n3.2 Compare Reliability by County Size\nYour Task: Use box plots to compare MOE across county size categories.\n\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = income_moe_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"Data Reliability by County Size Category\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend since x-axis is clear\n\n\n\n\n\n\n\n\n\n\n3.3 Challenge Exercise: Age Data Reliability\nYour Task: Analyze the reliability of median age data across counties.\nRequirements:\n\nCalculate MOE percentage for median age (median_ageM / median_ageE * 100)\nCreate a scatter plot showing population vs age MOE percentage\nUse purple points (color = \"purple\") with size = 2\nAdd a horizontal line at 5% MOE using geom_hline() with a blue dashed line\nUse theme_classic()instead of theme_minimal()\nCreate a boxplot comparing age MOE across the three population categories\n\n\n# Calculate MOE percentages\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    age_moe_pct = (median_ageM / median_ageE) * 100,\n    pop_category = case_when(\n      median_ageE &lt; 40 ~ \"Small (&lt;40)\",\n      median_ageE &lt; 50 ~ \"Medium (40-50)\",\n      TRUE ~ \"Large (&gt;50)\"\n    )\n  )\n\n# MOE by population size\nggplot(county_reliability) +\n  aes(x = median_ageE, y = age_moe_pct) +\n  geom_point(alpha = 0.7, color = \"purple\",size = 2) +\n  geom_hline(yintercept = 5, color = \"blue\", linetype = \"dashed\",size = 0.5 ) +\n  labs(\n    title = \"Data Reliability of median age data across\ncounties.\",\n    x = \"Median Age\",\n    y = \"Margin of Error (%)\",\n    caption = \"Red line = 5% reliability threshold\"\n  ) +\n  theme_classic() +\n  scale_x_continuous(labels = comma)\n\n\n\n\n\n\n\n\n\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = age_moe_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"age MOE across the three population categories\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend since x-axis is clear"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 4: Multiple Variables with Color and Faceting",
    "text": "Exercise 4: Multiple Variables with Color and Faceting\n\n4.1 Three-Variable Scatter Plot\nYour Task: Add median age as a color dimension to the population-income relationship.\n\n# Three-variable scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE, color = median_ageE) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_color_viridis_c(name = \"Median\\nAge\") +\n  labs(\n    title = \"Population, Income, and Age Patterns\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n4.2 Create Categories for Faceting\nYour Task: Create age categories and use faceting to compare patterns.\n\n# Create age categories and faceted plot\ncounty_faceted &lt;- county_data %&gt;%\n  mutate(\n    age_category = case_when(\n      median_ageE &lt; 40 ~ \"Young (&lt; 40)\",\n      median_ageE &lt; 45 ~ \"Middle-aged (40-45)\",\n      TRUE ~ \"Older (45+)\"\n    )\n  )\n\nggplot(county_faceted) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~age_category) +\n  labs(\n    title = \"Population-Income Relationship by Age Profile\",\n    x = \"Total Population\",\n    y = \"Median Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Do the relationships between population and income differ by age profile? Yes\nYour Task: Create a visualization using income categories and multiple aesthetic mappings.\nRequirements:\n\nCreate income categories: “Low” (&lt;$50k), “Middle” ($50k-$80k), “High” (&gt;$80k)\nMake a scatter plot with population (x) vs median age (y) - Color points by income category\nSize points by the margin of error for income (median_incomeM)\nUse the “Set2” color palette: scale_color_brewer(palette = \"Set2\") **note: you’ll need to load the RColorBrewer package for this`\nFacet by income category using facet_wrap()\nUse theme_bw() theme\n\n\n# Create income categories\ncounty_income_plot &lt;- county_data %&gt;%\n  mutate(\n    income_cat = case_when(\n      median_incomeE &lt; 50000 ~ \"Low (&lt;$50K)\",\n      median_incomeE &lt; 80000 ~ \"Middle ($50K–$80K)\",\n      TRUE ~ \"High (&gt;$80K)\"\n    )\n  )\n\n# Scatter plot\nggplot(county_income_plot) +\n  aes(\n    x = total_popE,\n    y = median_ageE,\n    color = income_cat,\n    size = median_incomeM\n  ) +\n  geom_point(alpha = 0.7) +\n  scale_color_brewer(palette = \"Set2\", name = \"Income Category\") +\n  scale_size_continuous(name = \"Income MOE\") +\n  labs(\n    title = \"Population vs Median Age by Income Category\",\n    x = \"Total Population\",\n    y = \"Median Age\"\n  ) +\n  facet_wrap(~ income_cat) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n  scale_x_continuous(labels = scales::comma)\n\n&lt;ScaleContinuousPosition&gt;\n Range:  \n Limits:    0 --    1"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 5: Data Joins and Integration",
    "text": "Exercise 5: Data Joins and Integration\n\n5.1 Get Additional Census Data\nYour Task: Load educational attainment data and join it with our existing data.\n\n# Get educational attainment data\neducation_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\"    # Bachelor's degree or higher\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  ) %&gt;%\n  select(GEOID, county_name, pct_college)\n\n# Check the data\nhead(education_data)\n\n# A tibble: 6 × 3\n  GEOID county_name                    pct_college\n  &lt;chr&gt; &lt;chr&gt;                                &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           13.9 \n2 42003 Allegheny County, Pennsylvania       25.4 \n3 42005 Armstrong County, Pennsylvania       12.7 \n4 42007 Beaver County, Pennsylvania          18.3 \n5 42009 Bedford County, Pennsylvania          9.73\n6 42011 Berks County, Pennsylvania           17.2 \n\n\n\n\n5.2 Join the Datasets\nYour Task: Join the education data with our main county dataset.\n\n# Perform the join\ncombined_data &lt;- county_data %&gt;%\n  left_join(education_data, by = \"GEOID\")\n\n# Check the join worked\ncat(\"Original data rows:\", nrow(county_data), \"\\n\")\n\nOriginal data rows: 67 \n\ncat(\"Combined data rows:\", nrow(combined_data), \"\\n\")\n\nCombined data rows: 67 \n\ncat(\"Missing education data:\", sum(is.na(combined_data$pct_college)), \"\\n\")\n\nMissing education data: 0 \n\n# View the combined data\nhead(combined_data)\n\n# A tibble: 6 × 11\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 4 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;\n\n\n\n\n5.3 Analyze the New Relationship\nYour Task: Explore the relationship between education and income.\n\n# Education vs Income scatter plot\nggplot(combined_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Education vs Income Across Counties\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\nedu_income_cor &lt;- cor(combined_data$pct_college, combined_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Education-Income Correlation:\", round(edu_income_cor, 3)))\n\n[1] \"Education-Income Correlation: 0.811\"\n\n\n\n\n5.4 Get Housing Data and Triple Join\nYour Task: Add housing cost data to create a three-way analysis.\n\n# Get housing cost data\nhousing_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_rent = \"B25058_001\",     # Median contract rent\n    median_home_value = \"B25077_001\" # Median value of owner-occupied units\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  select(GEOID, median_rent = median_rentE, median_home_value = median_home_valueE)\n\n# Join all three datasets\nfull_data &lt;- combined_data %&gt;%\n  left_join(housing_data, by = \"GEOID\")\n\n# Create a housing affordability measure\nfull_data &lt;- full_data %&gt;%\n  mutate(\n    rent_to_income = (median_rent * 12) / median_incomeE * 100,\n    income_category = case_when(\n      median_incomeE &lt; 50000 ~ \"Low Income\",\n      median_incomeE &lt; 80000 ~ \"Middle Income\",\n      TRUE ~ \"High Income\"\n    )\n  )\nhead(full_data)\n\n# A tibble: 6 × 15\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 8 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;, median_rent &lt;dbl&gt;,\n#   median_home_value &lt;dbl&gt;, rent_to_income &lt;dbl&gt;, income_category &lt;chr&gt;\n\n\n\n\n5.5 Advanced Multi-Variable Analysis\nYour Task: Create a comprehensive visualization showing multiple relationships.\n\n# Complex multi-variable plot\nggplot(full_data) +\n  aes(x = pct_college, y = rent_to_income, \n      color = income_category, size = total_popE) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Education, Housing Affordability, and Income Patterns\",\n    subtitle = \"Larger points = larger population\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Annual Rent as % of Median Income\",\n    color = \"Income Category\",\n    size = \"Population\"\n  ) +\n  theme_minimal() +\n  guides(size = guide_legend(override.aes = list(alpha = 1)))"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 6: Publication-Ready Visualization",
    "text": "Exercise 6: Publication-Ready Visualization\n\n6.1 Create a Policy-Focused Visualization\nYour Task: Combine multiple visualizations to tell a more complete story about county characteristics.\n\n# Create a multi-panel figure\nlibrary(patchwork)  # For combining plots\n\n# Plot 1: Income distribution\np1 &lt;- ggplot(full_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"A) Income Distribution\", \n       x = \"Median Income ($)\", y = \"Counties\") +\n  scale_x_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 2: Education vs Income\np2 &lt;- ggplot(full_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"B) Education vs Income\",\n       x = \"% College Educated\", y = \"Median Income ($)\") +\n  scale_y_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 3: Housing affordability by income category\np3 &lt;- ggplot(full_data) +\n  aes(x = income_category, y = rent_to_income, fill = income_category) +\n  geom_boxplot() +\n  labs(title = \"C) Housing Affordability by Income\",\n       x = \"Income Category\", y = \"Rent as % of Income\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Plot 4: Data reliability by population\np4 &lt;- ggplot(\n  county_data %&gt;%\n    mutate(income_moe_pct = (median_incomeM / median_incomeE) * 100)\n) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"D) Data Reliability\",\n       x = \"Population\", y = \"MOE (%)\") +\n  scale_x_continuous(labels = comma) +\n  theme_minimal()\n\n\n# Combine all plots\ncombined_plot &lt;- (p1 | p2) / (p3 | p4)\ncombined_plot + plot_annotation(\n  title = \"Pennsylvania County Analysis: Income, Education, and Housing Patterns\",\n  caption = \"Source: American Community Survey 2018-2022\"\n)"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations",
    "text": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations\nBackground: Research by Jurjevich et al. (2018) found that only 27% of planners warn users about unreliable ACS data, violating AICP ethical standards. In this exercise, you’ll practice the five research-based guidelines for ethical ACS data communication.\n\n7.1 Create Professional Data Tables with Uncertainty\nYour Task: Follow the Jurjevich et al. guidelines to create an ethical data presentation.\n\n# Get comprehensive data for ethical analysis\nethical_demo_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",   # Median household income\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\",   # Bachelor's degree or higher\n    total_pop = \"B01003_001\"        # Total population\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    # Calculate derived statistics\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    \n    # Calculate MOE for percentage using error propagation\n    pct_college_moe = pct_college * sqrt((bachelor_plusM/bachelor_plusE)^2 + (total_25plusM/total_25plusE)^2),\n    \n    # Calculate coefficient of variation for all key variables\n    income_cv = (median_incomeM / median_incomeE) * 100,\n    education_cv = (pct_college_moe / pct_college) * 100,\n    \n    # Create reliability categories based on CV\n    income_reliability = case_when(\n      income_cv &lt; 12 ~ \"High\",\n      income_cv &lt;= 40 ~ \"Moderate\", \n      TRUE ~ \"Low\"\n    ),\n    \n    education_reliability = case_when(\n      education_cv &lt; 12 ~ \"High\",\n      education_cv &lt;= 40 ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    ),\n    \n    # Create color coding for reliability\n    income_color = case_when(\n      income_reliability == \"High\" ~ \"🟢\",\n      income_reliability == \"Moderate\" ~ \"🟡\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    education_color = case_when(\n      education_reliability == \"High\" ~ \"🟢\",\n      education_reliability == \"Moderate\" ~ \"🟡\", \n      TRUE ~ \"🔴\"\n    ),\n    \n    # Clean county names\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  )\n\n# Create ethical data table focusing on least reliable estimates\nethical_data_table &lt;- ethical_demo_data %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color,\n         pct_college, pct_college_moe, education_cv, education_color) %&gt;%\n  arrange(desc(income_cv)) %&gt;%  # Show least reliable first\n  slice_head(n = 10)\n\n# Create professional table following guidelines\nlibrary(knitr)\nlibrary(kableExtra)\n\nethical_data_table %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \n                  \"CV (%)\", \"Reliability\"),\n    caption = \"Pennsylvania Counties: Median Household Income with Statistical Uncertainty\",\n    format.args = list(big.mark = \",\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = c(\"Coefficient of Variation (CV) indicates reliability:\",\n                \"🟢 High reliability (CV &lt; 12%)\",\n                \"🟡 Moderate reliability (CV 12-40%)\", \n                \"🔴 Low reliability (CV &gt; 40%)\",\n                \"Following Jurjevich et al. (2018) research recommendations\",\n                \"Source: American Community Survey 2018-2022 5-Year Estimates\"),\n    general_title = \"Notes:\"\n  )\n\n\nPennsylvania Counties: Median Household Income with Statistical Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nCV (%)\nReliability\n\n\n\n\nForest County, Pennsylvania\n46,188\n4,612\n9.985278\n🟢 |\n\n\nSullivan County, Pennsylvania\n62,910\n5,821\n9.252901\n🟢 |\n\n\nUnion County, Pennsylvania\n64,914\n4,753\n7.321995\n🟢 |\n\n\nMontour County, Pennsylvania\n72,626\n5,146\n7.085617\n🟢 |\n\n\nElk County, Pennsylvania\n61,672\n4,091\n6.633480\n🟢 |\n\n\nGreene County, Pennsylvania\n66,283\n4,247\n6.407374\n🟢 |\n\n\nCameron County, Pennsylvania\n46,186\n2,605\n5.640237\n🟢 |\n\n\nSnyder County, Pennsylvania\n65,914\n3,666\n5.561793\n🟢 |\n\n\nCarbon County, Pennsylvania\n64,538\n3,424\n5.305402\n🟢 |\n\n\nWarren County, Pennsylvania\n57,925\n3,005\n5.187743\n🟢 |\n\n\n\nNotes:\n\n\n\n\n\n\n Coefficient of Variation (CV) indicates reliability:\n\n\n\n\n\n\n 🟢 High reliability (CV &lt; 12%)\n\n\n\n\n\n\n 🟡 Moderate reliability (CV 12-40%)\n\n\n\n\n\n\n 🔴 Low reliability (CV &gt; 40%)\n\n\n\n\n\n\n Following Jurjevich et al. (2018) research recommendations\n\n\n\n\n\n\n Source: American Community Survey 2018-2022 5-Year Estimates\n\n\n\n\n\n\n\n\n\n\n\n\n7.3 Now try Census Tracts\n\n# Get census tract poverty data for Philadelphia\nphilly_poverty &lt;- get_acs(\n    geography = \"tract\",\n    variables = c(\n      poverty_pop = \"B17001_001\",     \n      poverty_below = \"B17001_002\"    \n    ),\n    state = \"PA\",\n    county = \"101\",\n    year = 2022,\n    output = \"wide\"\n  ) %&gt;%\n  filter(poverty_popE &gt; 0) %&gt;%  # Remove tracts with no poverty data\n  mutate(\n    # Calculate poverty rate and its MOE\n    poverty_rate = (poverty_belowE / poverty_popE) * 100,\n    \n    # MOE for derived percentage using error propagation\n    poverty_rate_moe = poverty_rate * sqrt((poverty_belowM/poverty_belowE)^2 + (poverty_popM/poverty_popE)^2),\n    \n    # Coefficient of variation\n    poverty_cv = (poverty_rate_moe / poverty_rate) * 100,\n    \n    # Reliability assessment\n    reliability = case_when(\n      poverty_cv &lt; 12 ~ \"High\",\n      poverty_cv &lt;= 40 ~ \"Moderate\",\n      poverty_cv &lt;= 75 ~ \"Low\",\n      TRUE ~ \"Very Low\"\n    ),\n    \n    # Color coding\n    reliability_color = case_when(\n      reliability == \"High\" ~ \"🟢\",\n      reliability == \"Moderate\" ~ \"🟡\",\n      reliability == \"Low\" ~ \"🟠\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    # Population size categories\n    pop_category = case_when(\n      poverty_popE &lt; 500 ~ \"Very Small (&lt;500)\",\n      poverty_popE &lt; 1000 ~ \"Small (500-1000)\",\n      poverty_popE &lt; 1500 ~ \"Medium (1000-1500)\",\n      TRUE ~ \"Large (1500+)\"\n    )\n  )\n\n# Check the data quality crisis at tracts\nreliability_summary &lt;- philly_poverty %&gt;%\n  count(reliability) %&gt;%\n  mutate(\n    percentage = round(n / sum(n) * 100, 1),\n    total_bg = sum(n)\n  )\n\nprint(\"Philadelphia Census Tract Poverty Data Reliability:\")\n\n[1] \"Philadelphia Census Tract Poverty Data Reliability:\"\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"Number of Tracts\", \"Percentage\", \"Total\"),\n    caption = \"The Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\"\n  ) %&gt;%\n  kable_styling()\n\n\nThe Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\n\n\nData Quality\nNumber of Tracts\nPercentage\nTotal\n\n\n\n\nLow\n295\n75.8\n389\n\n\nModerate\n53\n13.6\n389\n\n\nVery Low\n41\n10.5\n389\n\n\n\n\n\n\n# Show the most problematic estimates (following Guideline 3: provide context)\nworst_estimates &lt;- philly_poverty %&gt;%\n  filter(reliability %in% c(\"Low\", \"Very Low\")) %&gt;%\n  arrange(desc(poverty_cv)) %&gt;%\n  slice_head(n = 10)\n\nworst_estimates %&gt;%\n  select(GEOID, poverty_rate, poverty_rate_moe, poverty_cv, reliability_color, poverty_popE) %&gt;%\n  kable(\n    col.names = c(\"Tract\", \"Poverty Rate (%)\", \"MOE\", \"CV (%)\", \"Quality\", \"Pop Size\"),\n    caption = \"Guideline 3: Tracts with Least Reliable Poverty Estimates\",\n    digits = c(0, 1, 1, 1, 0, 0)\n  ) %&gt;%\n  kable_styling() %&gt;%\n  footnote(\n    general = c(\"These estimates should NOT be used for policy decisions\",\n                \"CV &gt; 75% indicates very low reliability\",\n                \"Recommend aggregation or alternative data sources\")\n  )\n\n\nGuideline 3: Tracts with Least Reliable Poverty Estimates\n\n\nTract\nPoverty Rate (%)\nMOE\nCV (%)\nQuality\nPop Size\n\n\n\n\n42101989100\n15.8\n45.2\n286.1\n🔴 |\n38|\n\n\n42101000101\n0.7\n1.1\n157.9\n🔴 |\n1947|\n\n\n42101980200\n37.9\n45.2\n119.4\n🔴 |\n66|\n\n\n42101023100\n3.8\n4.5\n119.4\n🔴 |\n1573|\n\n\n42101025600\n1.7\n2.0\n114.2\n🔴 |\n2642|\n\n\n42101014202\n1.7\n1.8\n107.0\n🔴 |\n2273|\n\n\n42101000403\n6.6\n6.7\n101.8\n🔴 |\n1047|\n\n\n42101026100\n4.7\n4.4\n95.0\n🔴 |\n2842|\n\n\n42101036502\n4.9\n4.7\n94.9\n🔴 |\n4284|\n\n\n42101032000\n21.8\n20.6\n94.8\n🔴 |\n7873|\n\n\n\nNote: \n\n\n\n\n\n\n\n These estimates should NOT be used for policy decisions\n\n\n\n\n\n\n\n CV &gt; 75% indicates very low reliability\n\n\n\n\n\n\n\n Recommend aggregation or alternative data sources"
  },
  {
    "objectID": "labs/lab1/week-03/script/week3_lab_exercise.html#key-references-and-acknowledgments",
    "href": "labs/lab1/week-03/script/week3_lab_exercise.html#key-references-and-acknowledgments",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Key References and Acknowledgments",
    "text": "Key References and Acknowledgments\nJurjevich, J. R., Griffin, A. L., Spielman, S. E., Folch, D. C., Merrick, M., & Nagle, N. N. (2018). Navigating statistical uncertainty: How urban and regional planners understand and work with American community survey (ACS) data for guiding policy. Journal of the American Planning Association, 84(2), 112-126.\nWalker, K. (2023). Analyzing US Census Data: Methods, Maps, and Models in R. Available at: https://walker-data.com/census-r/\nAI Acknowledgments: This lab was developed with coding assistance from Claude AI. I have run, reviewed, and edited the final version. Any remaining errors are my own."
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#what-well-cover",
    "href": "labs/lab1/week-03/lecture/week3.html#what-well-cover",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\nPart 1: Why Visualization Matters\n\nAnscombe’s Quartet and the limits of summary statistics\nVisualization in policy context\nConnection to algorithmic bias and data ethics\n\nPart 2: Grammar of Graphics\n\nggplot2 fundamentals\nAesthetic mappings and geoms\nLive demonstration\n\nPart 3: Exploratory Data Analysis\n\nEDA workflow and principles\nUnderstanding distributions and relationships\nCritical focus: Data quality and uncertainty\n\nPart 4: Data Joins & Integration\n\nCombining datasets with dplyr joins\n\nPart 5: Hands-On Lab\n\nGuided practice with census data\nCreate publication-ready visualizations\nPractice ethical data communication"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#opening-question",
    "href": "labs/lab1/week-03/lecture/week3.html#opening-question",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Opening Question",
    "text": "Opening Question\nThink about Assignment 1:\nYou created tables showing income reliability patterns across counties. But what if you needed to present these findings to:\n\nThe state legislature (2-minute briefing)\nCommunity advocacy groups\nLocal news reporters\n\nDiscussion: How might visual presentation change the impact of your analysis?"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "href": "labs/lab1/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Anscombe’s Quartet: The Famous Example",
    "text": "Anscombe’s Quartet: The Famous Example\nFour datasets with identical summary statistics:\n\nSame means (x̄ = 9, ȳ = 7.5)\nSame variances\nSame correlation (r = 0.816)\nSame regression line\n\nBut completely different patterns when visualized"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#the-policy-implications",
    "href": "labs/lab1/week-03/lecture/week3.html#the-policy-implications",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The Policy Implications",
    "text": "The Policy Implications\nWhy this matters for your work:\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nExample: A county with “average” income might have extreme inequality that algorithms would miss without visualization."
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "href": "labs/lab1/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connecting Week 2: Ethical Data Communication",
    "text": "Connecting Week 2: Ethical Data Communication\nFrom last week’s algorithmic bias discussion:\nResearch finding: Only 27% of planners warn users about unreliable ACS data - Most planners don’t report margins of error - Many lack training on statistical uncertainty - This violates AICP Code of Ethics\nYour responsibility:\n\nCreate honest, transparent visualizations\nAlways assess and communicate data quality\nConsider who might be harmed by uncertain data"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "href": "labs/lab1/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Bad Visualizations Have Real Consequences",
    "text": "Bad Visualizations Have Real Consequences\nCommon problems in government data presentation:\n\nMisleading scales or axes\nCherry-picked time periods\n\nHidden or ignored uncertainty\nMissing context about data reliability\n\nReal impact: The Jurjevich et al. study found that 72% of Portland census tracts had unreliable child poverty estimates, yet planners rarely communicated this uncertainty.\nResult: Poor policy decisions based on misunderstood data"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "href": "labs/lab1/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The ggplot2 Philosophy",
    "text": "The ggplot2 Philosophy\nGrammar of Graphics principles:\nData → Aesthetics → Geometries → Visual\n\nData: Your dataset (census data, survey responses, etc.)\nAesthetics: What variables map to visual properties (x, y, color, size)\nGeometries: How to display the data (points, bars, lines)\nAdditional layers: Scales, themes, facets, annotations"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#basic-ggplot2-structure",
    "href": "labs/lab1/week-03/lecture/week3.html#basic-ggplot2-structure",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Basic ggplot2 Structure",
    "text": "Basic ggplot2 Structure\nEvery ggplot has this pattern:\nggplot(data = your_data) +   aes(x = variable1, y = variable2) +   geom_something() +   additional_layers()\nYou build plots by adding layers with +"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "href": "labs/lab1/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Basic Scatter Plot",
    "text": "Live Demo: Basic Scatter Plot"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "href": "labs/lab1/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Aesthetic Mappings: The Key to ggplot2",
    "text": "Aesthetic Mappings: The Key to ggplot2\nAesthetics map data to visual properties:\n\nx, y - position\ncolor - point/line color\nfill - area fill color\n\nsize - point/line size\nshape - point shape\nalpha - transparency\n\nImportant: Aesthetics go inside aes(), constants go outside"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "href": "labs/lab1/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Improving Plots with Labels and Themes",
    "text": "Improving Plots with Labels and Themes"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#the-eda-mindset",
    "href": "labs/lab1/week-03/lecture/week3.html#the-eda-mindset",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The EDA Mindset",
    "text": "The EDA Mindset\nExploratory Data Analysis is detective work:\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\n\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nGoal: Understand your data before making decisions or building models"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "href": "labs/lab1/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA Workflow with Data Quality Focus",
    "text": "EDA Workflow with Data Quality Focus\nEnhanced process for policy analysis:\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#understanding-distributions",
    "href": "labs/lab1/week-03/lecture/week3.html#understanding-distributions",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Understanding Distributions",
    "text": "Understanding Distributions\nWhy distribution shape matters:\n\nWhat to look for: Skewness, outliers, multiple peaks, gaps"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#boxplots",
    "href": "labs/lab1/week-03/lecture/week3.html#boxplots",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Boxplots!",
    "text": "Boxplots!"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "href": "labs/lab1/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Critical: Data Quality Through Visualization",
    "text": "Critical: Data Quality Through Visualization\nResearch insight: Most planners don’t visualize or communicate uncertainty\n\nPattern: Smaller populations have higher uncertainty Ethical implication: These communities might be systematically undercounted"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "href": "labs/lab1/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Research-Based Recommendations for Planners",
    "text": "Research-Based Recommendations for Planners\nJurjevich et al. (2018): 5 Essential Guidelines for Using ACS Data\n\nReport the corresponding MOEs of ACS estimates - Always include margin of error values\nInclude a footnote when not reporting MOEs - Explicitly acknowledge omission\n\nProvide context for (un)reliability - Use coefficient of variation (CV):\n\nCV &lt; 12% = reliable (green coding)\nCV 12-40% = somewhat reliable (yellow)\nCV &gt; 40% = unreliable (red coding)\n\nReduce statistical uncertainty - Collapse data detail, aggregate geographies, use multi-year estimates\nAlways conduct statistical significance tests when comparing ACS estimates over time\n\nKey insight: These practices are not just technical best practices—they are ethical requirements under the AICP Code of Ethics"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#eda-for-policy-analysis",
    "href": "labs/lab1/week-03/lecture/week3.html#eda-for-policy-analysis",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA for Policy Analysis",
    "text": "EDA for Policy Analysis\nKey questions for census data:\n\nGeographic patterns: Are problems concentrated in certain areas?\nPopulation relationships: How does size affect data quality?\nDemographic patterns: Are certain communities systematically different?\nTemporal trends: How do patterns change over time?\nData integrity: Where might survey bias affect results?\nReliability assessment: Which estimates should we trust?"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#why-join-data",
    "href": "labs/lab1/week-03/lecture/week3.html#why-join-data",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Why Join Data?",
    "text": "Why Join Data?\nTo combining datasets of course:\n\nCensus demographics + Economic indicators\nSurvey responses + Geographic boundaries\n\nCurrent data + Historical trends\nAdministrative records + Survey data"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#types-of-joins-tabular",
    "href": "labs/lab1/week-03/lecture/week3.html#types-of-joins-tabular",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Types of Joins (tabular)",
    "text": "Types of Joins (tabular)\nFour main types in dplyr:\n\nleft_join() - Keep all rows from left dataset\nright_join() - Keep all rows from right dataset\n\ninner_join() - Keep only rows that match in both\nfull_join() - Keep all rows from both datasets\n\nMost common: left_join() to add columns to your main dataset"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "href": "labs/lab1/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Joining Census Tables",
    "text": "Live Demo: Joining Census Tables\n\n\n# A tibble: 6 × 6\n  GEOID NAME                    median_income income_moe college_pop college_moe\n  &lt;chr&gt; &lt;chr&gt;                           &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams County, Pennsylv…         78975       3334       10195         761\n2 42003 Allegheny County, Penn…         72537        869      229538        3311\n3 42005 Armstrong County, Penn…         61011       2202        6171         438\n4 42007 Beaver County, Pennsyl…         67194       1531       22588        1012\n5 42009 Bedford County, Pennsy…         58337       2606        3396         307\n6 42011 Berks County, Pennsylv…         74617       1191       50120        1654"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "href": "labs/lab1/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Checking Join Results and Data Quality",
    "text": "Checking Join Results and Data Quality\nAlways verify joins AND assess combined reliability:\n\n\nIncome data rows: 67 \n\n\nEducation data rows: 67 \n\n\nCombined data rows: 67 \n\n\n# A tibble: 1 × 2\n  missing_income missing_education\n           &lt;int&gt;             &lt;int&gt;\n1              0                 0\n\n\n# A tibble: 6 × 3\n  NAME                           income_cv college_cv\n  &lt;chr&gt;                              &lt;dbl&gt;      &lt;dbl&gt;\n1 Adams County, Pennsylvania          4.22       7.46\n2 Allegheny County, Pennsylvania      1.20       1.44\n3 Armstrong County, Pennsylvania      3.61       7.10\n4 Beaver County, Pennsylvania         2.28       4.48\n5 Bedford County, Pennsylvania        4.47       9.04\n6 Berks County, Pennsylvania          1.60       3.30"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#lab-structure-for-today",
    "href": "labs/lab1/week-03/lecture/week3.html#lab-structure-for-today",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Lab Structure for Today",
    "text": "Lab Structure for Today\nYou’ll work through six exercises:\n\nFinding Census Variables - Learn to search for the data you need\nSingle Variable EDA - Explore distributions and identify outliers\nTwo Variable Relationships - Create meaningful scatter plots\nData Quality Visualization - Practice ethical uncertainty communication\nMultiple Variables - Color, faceting, and complex relationships\nData Integration - Join datasets and create publication-ready visualizations"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#skills-youll-practice",
    "href": "labs/lab1/week-03/lecture/week3.html#skills-youll-practice",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Skills You’ll Practice",
    "text": "Skills You’ll Practice\nggplot2 fundamentals:\n\nScatter plots, histograms, boxplots\nAesthetic mappings and customization\nProfessional themes and labels\n\nEDA workflow:\n\nDistribution analysis\nOutlier detection\n\nPattern identification\n\nEthical data practice:\n\nVisualizing and reporting margins of error\nUsing coefficient of variation to assess reliability"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#connection-to-professional-ethics",
    "href": "labs/lab1/week-03/lecture/week3.html#connection-to-professional-ethics",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connection to Professional Ethics",
    "text": "Connection to Professional Ethics\nBy the end of today, you’ll be able to:\n\nVisually assess data quality issues\nCreate compelling presentations of demographic patterns\nCommunicate statistical uncertainty ethically and clearly\nIntegrate multiple data sources"
  },
  {
    "objectID": "labs/lab1/week-03/lecture/week3.html#questions-before-we-begin",
    "href": "labs/lab1/week-03/lecture/week3.html#questions-before-we-begin",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Questions Before We Begin?",
    "text": "Questions Before We Begin?\nReady for hands-on practice?\nRemember: Today’s skills build directly on Week 1-2 foundations:\n\nSame dplyr functions, now with visualization\nSame census data concepts, now with multiple tables\n\nLet’s create some beautiful graphs"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html",
    "href": "labs/lab0/script/lab0_template.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#data-structure-exploration",
    "href": "labs/lab0/script/lab0_template.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Check the column names\nnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50,000 - Columns:7 - Variable types: character - Problematic names:"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#tibble-vs-data-frame",
    "href": "labs/lab0/script/lab0_template.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\n#car_df\n\nQuestion: What differences do you notice in how they print?\nYour answer: [YOUR ANSWER]"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#selecting-columns",
    "href": "labs/lab0/script/lab0_template.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\n\n\n# Select Manufacturer, Price, and Fuel type\n\n\n# Challenge: Select all columns EXCEPT Engine Size"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#renaming-columns",
    "href": "labs/lab0/script/lab0_template.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\n\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: [YOUR ANSWER]"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#calculate-car-age",
    "href": "labs/lab0/script/lab0_template.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\n\n\n# Create a mileage_per_year column  \n\n\n# Look at your new columns\n#select(car_data, Model, year, age, Mileage, mileage_per_year)"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#categorize-cars",
    "href": "labs/lab0/script/lab0_template.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is mid-range (use case_when)\n\n\n# Check your categories select the new column and show it"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#basic-filtering",
    "href": "labs/lab0/script/lab0_template.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\n\n\n# Find cars with mileage less than 30,000\n\n\n# Find luxury cars (from price category) with low mileage"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#multiple-conditions",
    "href": "labs/lab0/script/lab0_template.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Honda OR Nissan\n\n\n# Find cars with price between $20,000 and $35,000\n\n\n# Find diesel cars less than 10 years old\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: [YOUR ANSWER]"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#basic-summaries",
    "href": "labs/lab0/script/lab0_template.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\n\n\n# Count cars by manufacturer"
  },
  {
    "objectID": "labs/lab0/script/lab0_template.html#categorical-summaries",
    "href": "labs/lab0/script/lab0_template.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories"
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#assignment-overview",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n# Load required packages\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(units)\nlibrary(stringr)\nlibrary(knitr)\nlibrary(scales)\nlibrary(readr)\nlibrary(kableExtra)\n\n\n# Load spatial data\npa_counties=st_read(\"/Users/cathy/GitHub/MUSA-5080-Fall-2025/lectures/week-04/data/Pennsylvania_County_Boundaries.shp\")\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `/Users/cathy/GitHub/MUSA-5080-Fall-2025/lectures/week-04/data/Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\npa_hospital=st_read(\"/Users/cathy/GitHub/MUSA-5080-Fall-2025/lectures/week-04/data/hospitals.geojson\")\n\nReading layer `hospitals' from data source \n  `/Users/cathy/GitHub/MUSA-5080-Fall-2025/lectures/week-04/data/hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state=\"PA\", cb=TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Check that all data loaded correctly\nhead(pa_counties)\n\nSimple feature collection with 6 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8905670 ymin: 4862594 xmax: -8317930 ymax: 5161279\nProjected CRS: WGS 84 / Pseudo-Mercator\n  OBJECTID MSLINK COUNTY_NAM COUNTY_NUM FIPS_COUNT COUNTY_ARE COUNTY_PER\n1      336     46 MONTGOMERY         46        091       &lt;NA&gt;       &lt;NA&gt;\n2      337      8   BRADFORD         08        015       &lt;NA&gt;       &lt;NA&gt;\n3      338      9      BUCKS         09        017       &lt;NA&gt;       &lt;NA&gt;\n4      339     58      TIOGA         58        117       &lt;NA&gt;       &lt;NA&gt;\n5      340     59      UNION         59        119       &lt;NA&gt;       &lt;NA&gt;\n6      341     60    VENANGO         60        121       &lt;NA&gt;       &lt;NA&gt;\n  NUMERIC_LA COUNTY_N_1 AREA_SQ_MI SOUND SPREAD_SHE IMAGE_NAME NOTE_FILE VIDEO\n1          5         46   487.4271  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n2          2          8  1161.3379  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n3          5          9   622.0836  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n4          2         58  1137.2480  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n5          2         59   319.1893  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n6          3         60   683.3676  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n  DISTRICT_N PA_CTY_COD MAINT_CTY_ DISTRICT_O                       geometry\n1         06         46          4        6-4 MULTIPOLYGON (((-8398884 48...\n2         03         08          9        3-9 MULTIPOLYGON (((-8558633 51...\n3         06         09          1        6-1 MULTIPOLYGON (((-8367360 49...\n4         03         59          7        3-7 MULTIPOLYGON (((-8558633 51...\n5         03         60          8        3-8 MULTIPOLYGON (((-8562865 49...\n6         01         61          5        1-5 MULTIPOLYGON (((-8870781 50...\n\nhead(pa_hospital)\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.27907 ymin: 39.80913 xmax: -75.17005 ymax: 40.24273\nGeodetic CRS:  WGS 84\n                 CHIEF_EXEC              CHIEF_EX_1\n1             Peter J Adamo               President\n2          Autumn DeShields Chief Executive Officer\n3              Shawn Parekh Chief Executive Officer\n4               DIANE HRITZ Chief Executive Officer\n5            Tim Harclerode Chief Executive Officer\n6 Richard McLaughlin MD MBA Chief Executive Officer\n                      FACILITY_U LONGITUDE       COUNTY\n1   https://www.phhealthcare.org -79.91131   Washington\n2      https://www.malvernbh.com -75.17005 Philadelphia\n3 https://roxboroughmemorial.com -75.20963 Philadelphia\n4     https://www.ashospital.net -80.27907   Washington\n5      https://www.conemaugh.org -79.02513     Somerset\n6        https://towerhealth.org -75.61213   Montgomery\n                               FACILITY_N                         STREET\n1               Penn Highlands Mon Valley         1163 Country Club Road\n2               MALVERN BEHAVIORAL HEALTH 1930 South Broad Street Unit 4\n3            Roxborough Memorial Hospital              5800 Ridge Avenue\n4              ADVANCED SURGICAL HOSPITAL       100 TRICH DRIVE\\nSUITE 1\n5 DLP Conemaugh Meyersdale Medical Center             200 Hospital Drive\n6                 Pottstown Hospital, LLC          1600 East High Street\n    CITY_OR_BO LATITUDE   TELEPHONE_ ZIP_CODE                   geometry\n1  Monongahela 40.18193 724-258-1000    15063 POINT (-79.91131 40.18193)\n2 Philadelphia 39.92619 610-480-8919    19145  POINT (-75.17005 39.9262)\n3 Philadelphia 40.02869 215-483-9900    19128 POINT (-75.20963 40.02869)\n4   WASHINGTON 40.15655   7248840710    15301 POINT (-80.27907 40.15655)\n5   Meyersdale 39.80913 814-634-5911    15552 POINT (-79.02513 39.80913)\n6    Pottstown 40.24273   6103277000    19464 POINT (-75.61213 40.24273)\n\nhead(census_tracts)\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -78.42478 ymin: 39.79351 xmax: -75.93766 ymax: 40.54328\nGeodetic CRS:  NAD83\n  STATEFP COUNTYFP TRACTCE              GEOIDFQ       GEOID   NAME\n1      42      001  031101 1400000US42001031101 42001031101 311.01\n2      42      013  100400 1400000US42013100400 42013100400   1004\n3      42      013  100500 1400000US42013100500 42013100500   1005\n4      42      013  100800 1400000US42013100800 42013100800   1008\n5      42      013  101900 1400000US42013101900 42013101900   1019\n6      42      011  011200 1400000US42011011200 42011011200    112\n             NAMELSAD STUSPS   NAMELSADCO   STATE_NAME LSAD   ALAND AWATER\n1 Census Tract 311.01     PA Adams County Pennsylvania   CT 3043185      0\n2   Census Tract 1004     PA Blair County Pennsylvania   CT  993724      0\n3   Census Tract 1005     PA Blair County Pennsylvania   CT 1130204      0\n4   Census Tract 1008     PA Blair County Pennsylvania   CT  996553      0\n5   Census Tract 1019     PA Blair County Pennsylvania   CT  573726      0\n6    Census Tract 112     PA Berks County Pennsylvania   CT 1539365   9308\n                        geometry\n1 MULTIPOLYGON (((-77.03108 3...\n2 MULTIPOLYGON (((-78.42478 4...\n3 MULTIPOLYGON (((-78.41661 4...\n4 MULTIPOLYGON (((-78.41067 4...\n5 MULTIPOLYGON (((-78.40836 4...\n6 MULTIPOLYGON (((-75.95433 4...\n\n\n\n\nCRS summary — Counties: WGS 84 / Pseudo-Mercator ; Hospitals: WGS 84 ; Tracts: NAD83 \n\n\nQuestions to answer: - How many hospitals are in your dataset? 219 - How many census tracts? 3445 - What coordinate reference system is each dataset in? Counties: WGS 84 / Pseudo-Mercator ; Hospitals: WGS 84 ; Tracts: NAD83 —\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n# Get demographic data from ACS\ndata=c(over65=\"DP05_0024\",income=\"DP03_0062\",population=\"DP02_0018\")\nd1=get_acs(\n  state = \"PA\",\n  geography = \"tract\",\n  year = 2023,\n  survey = \"acs5\",\n  variables = data,\n  geometry = TRUE,\n  output=\"wide\"\n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Join to tract boundaries\nd1 &lt;- left_join(census_tracts, st_drop_geometry(d1), by = \"GEOID\")\n\nhead(d1)\n\nSimple feature collection with 6 features and 20 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -78.42478 ymin: 39.79351 xmax: -75.93766 ymax: 40.54328\nGeodetic CRS:  NAD83\n  STATEFP COUNTYFP TRACTCE              GEOIDFQ       GEOID NAME.x\n1      42      001  031101 1400000US42001031101 42001031101 311.01\n2      42      013  100400 1400000US42013100400 42013100400   1004\n3      42      013  100500 1400000US42013100500 42013100500   1005\n4      42      013  100800 1400000US42013100800 42013100800   1008\n5      42      013  101900 1400000US42013101900 42013101900   1019\n6      42      011  011200 1400000US42011011200 42011011200    112\n             NAMELSAD STUSPS   NAMELSADCO   STATE_NAME LSAD   ALAND AWATER\n1 Census Tract 311.01     PA Adams County Pennsylvania   CT 3043185      0\n2   Census Tract 1004     PA Blair County Pennsylvania   CT  993724      0\n3   Census Tract 1005     PA Blair County Pennsylvania   CT 1130204      0\n4   Census Tract 1008     PA Blair County Pennsylvania   CT  996553      0\n5   Census Tract 1019     PA Blair County Pennsylvania   CT  573726      0\n6    Census Tract 112     PA Berks County Pennsylvania   CT 1539365   9308\n                                           NAME.y over65E over65M incomeE\n1 Census Tract 311.01; Adams County; Pennsylvania     995     261   64985\n2   Census Tract 1004; Blair County; Pennsylvania     255     101   68929\n3   Census Tract 1005; Blair County; Pennsylvania     447      89   50241\n4   Census Tract 1008; Blair County; Pennsylvania     243      75   73625\n5   Census Tract 1019; Blair County; Pennsylvania     578     121   16547\n6    Census Tract 112; Berks County; Pennsylvania     496     111   61974\n  incomeM populationE populationM                       geometry\n1   10752        4865         412 MULTIPOLYGON (((-77.03108 3...\n2   17235        1660         247 MULTIPOLYGON (((-78.42478 4...\n3    5060        3360         562 MULTIPOLYGON (((-78.41661 4...\n4   18161        1490         333 MULTIPOLYGON (((-78.41067 4...\n5    1547        1247         184 MULTIPOLYGON (((-78.40836 4...\n6    6269        4123          38 MULTIPOLYGON (((-75.95433 4...\n\nst_geometry_type(d1)\n\n   [1] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n   [6] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [11] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [16] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [21] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [26] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [31] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [36] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [41] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [46] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [51] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [56] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [61] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [66] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [71] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [76] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [81] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [86] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [91] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n  [96] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [101] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [106] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [111] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [116] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [121] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [126] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [131] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [136] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [141] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [146] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [151] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [156] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [161] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [166] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [171] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [176] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [181] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [186] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [191] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [196] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [201] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [206] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [211] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [216] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [221] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [226] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [231] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [236] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [241] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [246] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [251] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [256] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [261] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [266] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [271] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [276] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [281] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [286] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [291] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [296] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [301] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [306] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [311] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [316] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [321] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [326] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [331] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [336] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [341] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [346] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [351] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [356] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [361] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [366] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [371] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [376] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [381] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [386] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [391] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [396] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [401] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [406] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [411] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [416] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [421] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [426] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [431] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [436] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [441] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [446] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [451] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [456] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [461] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [466] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [471] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [476] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [481] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [486] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [491] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [496] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [501] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [506] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [511] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [516] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [521] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [526] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [531] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [536] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [541] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [546] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [551] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [556] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [561] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [566] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [571] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [576] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [581] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [586] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [591] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [596] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [601] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [606] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [611] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [616] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [621] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [626] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [631] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [636] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [641] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [646] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [651] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [656] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [661] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [666] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [671] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [676] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [681] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [686] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [691] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [696] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [701] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [706] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [711] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [716] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [721] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [726] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [731] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [736] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [741] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [746] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [751] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [756] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [761] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [766] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [771] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [776] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [781] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [786] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [791] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [796] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [801] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [806] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [811] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [816] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [821] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [826] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [831] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [836] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [841] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [846] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [851] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [856] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [861] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [866] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [871] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [876] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [881] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [886] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [891] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [896] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [901] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [906] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [911] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [916] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [921] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [926] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [931] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [936] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [941] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [946] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [951] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [956] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [961] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [966] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [971] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [976] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [981] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [986] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [991] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n [996] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1001] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1006] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1011] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1016] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1021] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1026] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1031] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1036] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1041] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1046] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1051] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1056] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1061] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1066] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1071] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1076] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1081] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1086] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1091] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1096] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1101] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1106] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1111] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1116] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1121] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1126] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1131] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1136] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1141] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1146] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1151] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1156] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1161] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1166] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1171] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1176] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1181] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1186] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1191] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1196] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1201] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1206] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1211] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1216] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1221] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1226] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1231] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1236] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1241] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1246] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1251] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1256] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1261] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1266] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1271] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1276] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1281] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1286] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1291] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1296] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1301] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1306] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1311] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1316] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1321] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1326] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1331] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1336] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1341] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1346] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1351] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1356] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1361] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1366] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1371] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1376] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1381] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1386] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1391] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1396] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1401] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1406] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1411] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1416] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1421] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1426] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1431] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1436] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1441] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1446] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1451] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1456] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1461] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1466] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1471] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1476] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1481] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1486] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1491] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1496] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1501] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1506] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1511] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1516] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1521] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1526] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1531] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1536] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1541] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1546] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1551] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1556] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1561] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1566] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1571] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1576] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1581] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1586] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1591] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1596] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1601] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1606] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1611] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1616] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1621] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1626] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1631] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1636] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1641] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1646] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1651] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1656] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1661] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1666] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1671] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1676] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1681] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1686] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1691] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1696] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1701] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1706] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1711] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1716] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1721] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1726] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1731] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1736] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1741] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1746] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1751] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1756] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1761] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1766] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1771] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1776] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1781] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1786] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1791] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1796] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1801] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1806] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1811] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1816] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1821] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1826] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1831] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1836] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1841] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1846] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1851] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1856] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1861] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1866] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1871] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1876] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1881] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1886] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1891] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1896] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1901] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1906] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1911] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1916] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1921] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1926] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1931] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1936] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1941] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1946] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1951] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1956] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1961] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1966] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1971] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1976] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1981] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1986] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1991] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[1996] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2001] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2006] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2011] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2016] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2021] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2026] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2031] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2036] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2041] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2046] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2051] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2056] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2061] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2066] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2071] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2076] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2081] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2086] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2091] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2096] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2101] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2106] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2111] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2116] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2121] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2126] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2131] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2136] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2141] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2146] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2151] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2156] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2161] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2166] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2171] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2176] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2181] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2186] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2191] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2196] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2201] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2206] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2211] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2216] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2221] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2226] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2231] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2236] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2241] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2246] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2251] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2256] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2261] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2266] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2271] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2276] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2281] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2286] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2291] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2296] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2301] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2306] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2311] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2316] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2321] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2326] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2331] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2336] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2341] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2346] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2351] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2356] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2361] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2366] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2371] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2376] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2381] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2386] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2391] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2396] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2401] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2406] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2411] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2416] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2421] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2426] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2431] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2436] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2441] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2446] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2451] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2456] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2461] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2466] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2471] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2476] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2481] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2486] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2491] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2496] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2501] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2506] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2511] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2516] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2521] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2526] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2531] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2536] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2541] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2546] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2551] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2556] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2561] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2566] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2571] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2576] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2581] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2586] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2591] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2596] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2601] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2606] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2611] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2616] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2621] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2626] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2631] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2636] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2641] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2646] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2651] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2656] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2661] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2666] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2671] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2676] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2681] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2686] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2691] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2696] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2701] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2706] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2711] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2716] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2721] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2726] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2731] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2736] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2741] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2746] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2751] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2756] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2761] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2766] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2771] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2776] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2781] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2786] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2791] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2796] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2801] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2806] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2811] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2816] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2821] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2826] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2831] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2836] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2841] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2846] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2851] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2856] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2861] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2866] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2871] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2876] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2881] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2886] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2891] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2896] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2901] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2906] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2911] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2916] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2921] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2926] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2931] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2936] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2941] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2946] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2951] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2956] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2961] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2966] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2971] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2976] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2981] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2986] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2991] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[2996] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3001] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3006] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3011] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3016] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3021] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3026] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3031] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3036] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3041] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3046] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3051] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3056] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3061] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3066] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3071] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3076] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3081] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3086] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3091] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3096] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3101] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3106] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3111] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3116] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3121] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3126] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3131] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3136] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3141] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3146] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3151] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3156] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3161] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3166] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3171] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3176] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3181] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3186] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3191] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3196] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3201] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3206] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3211] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3216] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3221] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3226] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3231] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3236] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3241] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3246] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3251] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3256] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3261] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3266] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3271] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3276] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3281] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3286] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3291] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3296] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3301] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3306] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3311] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3316] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3321] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3326] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3331] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3336] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3341] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3346] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3351] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3356] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3361] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3366] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3371] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3376] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3381] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3386] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3391] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3396] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3401] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3406] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3411] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3416] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3421] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3426] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3431] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3436] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[3441] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n\n\n# Check for missing income values\nsum(is.na(d1$incomeE))\n\n[1] 65\n\n# Calculate the median income across all tracts\nmedian_income &lt;- median(d1$incomeE, na.rm = TRUE)\nmedian_income\n\n[1] 72943.5\n\n\nQuestions to answer: - What year of ACS data are you using? 2023 ACS 5-Year Estimates. - How many tracts have missing income data? 65 - What is the median income across all PA census tracts? 72943.5 —\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\nsummary(d1)\n\n   STATEFP            COUNTYFP           TRACTCE            GEOIDFQ         \n Length:3445        Length:3445        Length:3445        Length:3445       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    GEOID              NAME.x            NAMELSAD            STUSPS         \n Length:3445        Length:3445        Length:3445        Length:3445       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  NAMELSADCO         STATE_NAME            LSAD               ALAND          \n Length:3445        Length:3445        Length:3445        Min.   :7.224e+04  \n Class :character   Class :character   Class :character   1st Qu.:1.320e+06  \n Mode  :character   Mode  :character   Mode  :character   Median :4.183e+06  \n                                                          Mean   :3.364e+07  \n                                                          3rd Qu.:2.858e+07  \n                                                          Max.   :1.024e+09  \n                                                                             \n     AWATER            NAME.y             over65E          over65M     \n Min.   :       0   Length:3445        Min.   :   0.0   Min.   :  3.0  \n 1st Qu.:       0   Class :character   1st Qu.: 429.0   1st Qu.: 94.0  \n Median :   13905   Mode  :character   Median : 665.0   Median :138.0  \n Mean   :  433440                      Mean   : 718.8   Mean   :156.3  \n 3rd Qu.:  240806                      3rd Qu.: 945.0   3rd Qu.:199.0  \n Max.   :29792870                      Max.   :2541.0   Max.   :826.0  \n                                                                       \n    incomeE          incomeM        populationE     populationM    \n Min.   : 13307   Min.   :   472   Min.   :    0   Min.   :   7.0  \n 1st Qu.: 57864   1st Qu.:  9324   1st Qu.: 2508   1st Qu.: 247.0  \n Median : 72944   Median : 13780   Median : 3524   Median : 375.0  \n Mean   : 80731   Mean   : 16219   Mean   : 3647   Mean   : 402.5  \n 3rd Qu.: 96691   3rd Qu.: 20141   3rd Qu.: 4658   3rd Qu.: 527.0  \n Max.   :250001   Max.   :208341   Max.   :10388   Max.   :2092.0  \n NA's   :65       NA's   :73                                       \n          geometry   \n MULTIPOLYGON :3445  \n epsg:4269    :   0  \n +proj=long...:   0  \n                     \n                     \n                     \n                     \n\nvulnerable=d1%&gt;%\n  filter(over65E&gt;945|incomeE&lt;32150\n  )\nnum_vulnerable &lt;- nrow(vulnerable)\ntotal_tracts &lt;- nrow(d1)\npercent_vulnerable &lt;- round((num_vulnerable / total_tracts) * 100, 1)\n\ncat(\"Low income threshold: $32,150\\n\")\n\nLow income threshold: $32,150\n\ncat(\"Elderly population threshold: 945 people\\n\")\n\nElderly population threshold: 945 people\n\ncat(\"Number of vulnerable tracts:\", num_vulnerable, \"\\n\")\n\nNumber of vulnerable tracts: 974 \n\ncat(\"Total tracts:\", total_tracts, \"\\n\")\n\nTotal tracts: 3445 \n\ncat(\"Percentage of vulnerable tracts:\", percent_vulnerable, \"%\\n\")\n\nPercentage of vulnerable tracts: 28.3 %\n\n\nQuestions to answer: - What income threshold did you choose and why? I chose an income threshold of $32,150, based on the 2023 U.S. Federal Poverty Guideline. This provides a policy-relevant definition of low-income households.\n\nWhat elderly population threshold did you choose and why? I selected 945 residents aged 65 and over as the threshold, which represents roughly the top 25% of tracts by elderly population size. These areas are likely to face higher demand for healthcare and mobility support.\nHow many tracts meet your vulnerability criteria? 974 tracts meet at least one of the criteria.\nWhat percentage of PA census tracts are considered vulnerable by your definition? Approximately 28.3% of all Pennsylvania census tracts are considered vulnerable. —\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n# Transform to appropriate projected CRS\nproj_crs &lt;- 3365\n\nvulnerable_proj &lt;- vulnerable %&gt;%\n  st_transform(proj_crs)\n\nhosp_proj &lt;- pa_hospital %&gt;%\n  st_transform(proj_crs)\n\nvulner_centroid &lt;- st_centroid(vulnerable_proj)\n\ndist &lt;- vulner_centroid %&gt;%\n  mutate(\n    dist_meters = apply(st_distance(vulner_centroid, hosp_proj), 1, min),\n    dist_miles = dist_meters * 0.000621371\n  )\n\navg_dist &lt;- mean(dist$dist_miles, na.rm = TRUE)\nmax_dist &lt;- max(dist$dist_miles, na.rm = TRUE)\nnum_over15 &lt;- sum(dist$dist_miles &gt; 15)\n\ncat(\"Average distance to nearest hospital (miles):\", round(avg_dist, 2), \"\\n\")\n\nAverage distance to nearest hospital (miles): 13.72 \n\ncat(\"Maximum distance (miles):\", round(max_dist, 2), \"\\n\")\n\nMaximum distance (miles): 85.17 \n\ncat(\"Number of vulnerable tracts &gt;15 miles from a hospital:\", num_over15, \"\\n\")\n\nNumber of vulnerable tracts &gt;15 miles from a hospital: 314 \n\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? The average distance to the nearest hospital for vulnerable tracts is approximately 13.72 miles. - What is the maximum distance? The maximum distance from a vulnerable tract to the nearest hospital is 85.18 miles. - How many vulnerable tracts are more than 15 miles from the nearest hospital? There are 314 tracts located more than 15 miles from the nearest hospital.\nProjection used: EPSG:3365 – NAD83 / Pennsylvania South. It was chosen because it preserves local distances and areas for Pennsylvania, ensuring accurate measurement results in meters rather than degrees. —\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n#### Step 5: Identify Underserved Areas \n\nunderserved &lt;- dist %&gt;%\n  mutate(underserved = ifelse(dist_miles &gt; 15, TRUE, FALSE))\n\nnum_underserved &lt;- sum(underserved$underserved, na.rm = TRUE)\ntotal_vulnerable &lt;- nrow(underserved)\nperc_underserved &lt;- round((num_underserved / total_vulnerable) * 100, 1)\n\ncat(\"Number of underserved tracts:\", num_underserved, \"\\n\")\n\nNumber of underserved tracts: 314 \n\ncat(\"Percentage of vulnerable tracts that are underserved:\", perc_underserved, \"%\\n\")\n\nPercentage of vulnerable tracts that are underserved: 32.2 %\n\n\nQuestions to answer: - How many tracts are underserved? 314 tracts are considered underserved. - What percentage of vulnerable tracts are underserved? 32.2% of all vulnerable tracts are underserved. - Does this surprise you? Why or why not? This result is not surprising. Many underserved tracts are likely located in rural or remote counties, particularly in central and northern Pennsylvania, where hospitals are sparse and travel distances are long. In contrast, urban areas such as Philadelphia and Pittsburgh have higher hospital densities, providing better healthcare accessibility. —\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\npa_counties_proj &lt;- st_transform(pa_counties, st_crs(underserved))\ntracts_with_county &lt;- st_join(underserved, pa_counties_proj, join = st_intersects, left = FALSE)\n\nst_crs(underserved)$input\n\n[1] \"EPSG:3365\"\n\nst_crs(pa_counties_proj)$input\n\n[1] \"EPSG:3365\"\n\ntracts_with_county &lt;- st_join(underserved, pa_counties_proj, join = st_intersects, left = FALSE)\n\nnames(pa_counties)\n\n [1] \"OBJECTID\"   \"MSLINK\"     \"COUNTY_NAM\" \"COUNTY_NUM\" \"FIPS_COUNT\"\n [6] \"COUNTY_ARE\" \"COUNTY_PER\" \"NUMERIC_LA\" \"COUNTY_N_1\" \"AREA_SQ_MI\"\n[11] \"SOUND\"      \"SPREAD_SHE\" \"IMAGE_NAME\" \"NOTE_FILE\"  \"VIDEO\"     \n[16] \"DISTRICT_N\" \"PA_CTY_COD\" \"MAINT_CTY_\" \"DISTRICT_O\" \"geometry\"  \n\ncounty_summary &lt;- tracts_with_county %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    vulnerable_tracts = n(),\n    underserved_tracts = sum(underserved, na.rm = TRUE),\n    perc_underserved = round((underserved_tracts / vulnerable_tracts) * 100, 1),\n    avg_distance_miles = round(mean(dist_miles, na.rm = TRUE), 2),\n    total_vulnerable_pop = sum(populationE, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(perc_underserved))\n\nhead(county_summary, 10)\n\n# A tibble: 10 × 6\n   COUNTY_NAM     vulnerable_tracts underserved_tracts perc_underserved\n   &lt;chr&gt;                      &lt;int&gt;              &lt;int&gt;            &lt;dbl&gt;\n 1 CLARION                        1                  1              100\n 2 GREENE                         1                  1              100\n 3 JUNIATA                        1                  1              100\n 4 MCKEAN                         1                  1              100\n 5 NORTHUMBERLAND                 5                  5              100\n 6 PIKE                           2                  2              100\n 7 SNYDER                         5                  5              100\n 8 TIOGA                          4                  4              100\n 9 VENANGO                        3                  3              100\n10 WARREN                         1                  1              100\n# ℹ 2 more variables: avg_distance_miles &lt;dbl&gt;, total_vulnerable_pop &lt;dbl&gt;\n\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? NORTHUMBERLAND,SNYDER,TIOGA,VENANGO,PIKE\n\nWhich counties have the most vulnerable people living far from hospitals? ALLEGHENY,PHILADELPHIA,BUCKS,MONTGOMERY,LANCASTER\nAre there any patterns in where underserved counties are located? Underserved counties are mainly located in rural northern and central Pennsylvania, where low population density and limited hospital infrastructure create significant barriers to healthcare access. —\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\n\npriority_counties &lt;- county_summary %&gt;%\narrange(desc(perc_underserved), desc(avg_distance_miles)) %&gt;%\nslice_head(n = 10) %&gt;%\nmutate(\nperc_underserved = paste0(perc_underserved, \"%\"),\navg_distance_miles = round(avg_distance_miles, 2),\ntotal_vulnerable_pop = formatC(total_vulnerable_pop, format = \"d\", big.mark = \",\")\n)\n\nlibrary(kableExtra)\nkable(\n  priority_counties,\n  col.names = c(\"County\", \"Vulnerable Tracts\", \"Underserved Tracts\", \"% Underserved\", \n                \"Avg Distance (miles)\", \"Total Vulnerable Population\"),\n  caption = \"Table 1. Top 10 Pennsylvania Counties with the Greatest Need for Healthcare Investment\",\n  align = \"c\"\n) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    full_width = FALSE,\n    position = \"center\",\n    font_size = 13\n  ) %&gt;%\n  row_spec(0, bold = TRUE, background = \"darkgrey\", color = \"white\") %&gt;% \n  row_spec(1:10, background = \"#f8f9fa\") %&gt;%                             \n  add_header_above(c(\" \" = 1, \"Tract Counts\" = 2, \"Accessibility & Population\" = 3)) \n\n\n\nTable 1. Top 10 Pennsylvania Counties with the Greatest Need for Healthcare Investment\n\n\n\n\n\n\n\n\n\n\n\n\nTract Counts\n\n\nAccessibility & Population\n\n\n\nCounty\nVulnerable Tracts\nUnderserved Tracts\n% Underserved\nAvg Distance (miles)\nTotal Vulnerable Population\n\n\n\n\nPIKE\n2\n2\n100%\n77.36\n8,286\n\n\nWYOMING\n1\n1\n100%\n63.59\n4,185\n\n\nMCKEAN\n1\n1\n100%\n46.55\n5,054\n\n\nSNYDER\n5\n5\n100%\n44.30\n27,456\n\n\nNORTHUMBERLAND\n5\n5\n100%\n35.49\n27,608\n\n\nGREENE\n1\n1\n100%\n30.53\n5,359\n\n\nJUNIATA\n1\n1\n100%\n28.96\n5,560\n\n\nVENANGO\n3\n3\n100%\n28.69\n12,686\n\n\nTIOGA\n4\n4\n100%\n28.15\n17,895\n\n\nWARREN\n1\n1\n100%\n27.56\n5,066\n\n\n\n\n\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-2-comprehensive-visualization",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(knitr)\ntmap_mode(\"plot\")\ntm_shape(pa_counties_proj %&gt;% \n           left_join(county_summary, by = c(\"COUNTY_NAM\" = \"COUNTY_NAM\"))) +\n  tm_polygons(\n    col = \"avg_distance_miles\",     \n    palette = \"Blues\",                \n    style = \"quantile\",\n    n = 5,\n    title = \"Avg. Distance to Hospital (miles)\"\n  ) +\n  tm_borders(col = \"gray50\", lwd = 0.5) +\n  tm_layout(\n    legend.outside = TRUE,\n    frame = FALSE,\n    bg.color = \"white\"\n  )\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)\n\ncounty_map &lt;- pa_counties %&gt;%\n  st_transform(3365) %&gt;%\n  left_join(county_summary, by = c(\"COUNTY_NAM\" = \"COUNTY_NAM\"))\n\nhospital_points &lt;- pa_hospital %&gt;%\n  st_transform(3365)\n\nggplot() +\n  geom_sf(data = county_map, aes(fill = perc_underserved), color = \"white\", size = 0.3) +\n  geom_sf(data = hospital_points, color = \"pink\", size = 1, alpha = 0.7) +\n  scale_fill_gradient(\n    name = \"% Underserved\\nVulnerable Tracts\",\n    low = \"lightgrey\", high = \"black\",\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  labs(\n    title = \"Healthcare Accessibility Across Pennsylvania Counties\",\n    subtitle = \"Percentage of vulnerable tracts underserved and hospital locations\",\n    caption = \"Data source: 2023 ACS 5-Year Estimates & Pennsylvania Department of Health\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, margin = margin(b = 10)),\n    plot.caption = element_text(size = 9, color = \"gray40\"),\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create a clean dataframe of distances\ndist_df &lt;- dist %&gt;%\n  st_drop_geometry() %&gt;%\n  select(dist_miles)\n\nggplot(dist_df, aes(x = dist_miles)) +\n  geom_histogram(\n    bins = 30,\n    fill = \"lightpink\",\n    color = \"white\",\n    alpha = 0.8\n  ) +\n  labs(\n    title = \"Distribution of Distances to the Nearest Hospital (Vulnerable Tracts)\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Number of Vulnerable Census Tracts\",\n    caption = \"Most vulnerable tracts are within 15 miles of a hospital, but a right tail shows rural access challenges.\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(size = 15, face = \"bold\"),\n    axis.title = element_text(face = \"bold\"),\n    plot.caption = element_text(size = 10, color = \"gray40\")\n  )\n\n\n\n\n\n\n\n\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\nSchool Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you’re feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)\n\nschools &lt;- st_read(\"/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/Schools.geojson\")\n\nReading layer `Schools' from data source \n  `/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/Schools.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 495 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.2665 ymin: 39.90781 xmax: -74.97057 ymax: 40.12974\nGeodetic CRS:  WGS 84\n\ncrime &lt;- st_read(\"/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/incidents_part1_part2/incidents_part1_part2.shp\")\n\nReading layer `incidents_part1_part2' from data source \n  `/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/incidents_part1_part2/incidents_part1_part2.shp' \n  using driver `ESRI Shapefile'\nreplacing null geometries with empty geometries\nSimple feature collection with 120567 features and 13 fields (with 4991 geometries empty)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.27421 ymin: 5.684342e-14 xmax: 5.684342e-14 ymax: 40.13683\nGeodetic CRS:  WGS 84\n\nbike_network &lt;- st_read(\"/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/PhiladelphiaBikeNetwork_SupportingDatasets201209/BikeNetwork_SupportingDatasets201209/PhiladelphiaBikeNetwork201204.shp\")\n\nReading layer `PhiladelphiaBikeNetwork201204' from data source \n  `/Users/cathy/GitHub/portfolio-setup-uxiaoo22/Assignments/Assignment_2/PhiladelphiaBikeNetwork_SupportingDatasets201209/BikeNetwork_SupportingDatasets201209/PhiladelphiaBikeNetwork201204.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3719 features and 7 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 2663613 ymin: 207711.1 xmax: 2747362 ymax: 299020.2\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\n# Convert all to same projected CRS\nschools &lt;- st_transform(schools, 2272)\ncrime &lt;- st_transform(crime, 2272)\nbike_network &lt;- st_transform(bike_network, 2272)\n\n\nst_crs(schools)\n\nCoordinate Reference System:\n  User input: EPSG:2272 \n  wkt:\nPROJCRS[\"NAD83 / Pennsylvania South (ftUS)\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"SPCS83 Pennsylvania South zone (US survey foot)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",39.3333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-77.75,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",40.9666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",39.9333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1968500,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United States (USA) - Pennsylvania - counties of Adams; Allegheny; Armstrong; Beaver; Bedford; Berks; Blair; Bucks; Butler; Cambria; Chester; Cumberland; Dauphin; Delaware; Fayette; Franklin; Fulton; Greene; Huntingdon; Indiana; Juniata; Lancaster; Lawrence; Lebanon; Lehigh; Mifflin; Montgomery; Northampton; Perry; Philadelphia; Schuylkill; Snyder; Somerset; Washington; Westmoreland; York.\"],\n        BBOX[39.71,-80.53,41.18,-74.72]],\n    ID[\"EPSG\",2272]]\n\nst_crs(crime)\n\nCoordinate Reference System:\n  User input: EPSG:2272 \n  wkt:\nPROJCRS[\"NAD83 / Pennsylvania South (ftUS)\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"SPCS83 Pennsylvania South zone (US survey foot)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",39.3333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-77.75,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",40.9666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",39.9333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1968500,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United States (USA) - Pennsylvania - counties of Adams; Allegheny; Armstrong; Beaver; Bedford; Berks; Blair; Bucks; Butler; Cambria; Chester; Cumberland; Dauphin; Delaware; Fayette; Franklin; Fulton; Greene; Huntingdon; Indiana; Juniata; Lancaster; Lawrence; Lebanon; Lehigh; Mifflin; Montgomery; Northampton; Perry; Philadelphia; Schuylkill; Snyder; Somerset; Washington; Westmoreland; York.\"],\n        BBOX[39.71,-80.53,41.18,-74.72]],\n    ID[\"EPSG\",2272]]\n\nst_crs(bike_network)\n\nCoordinate Reference System:\n  User input: EPSG:2272 \n  wkt:\nPROJCRS[\"NAD83 / Pennsylvania South (ftUS)\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"SPCS83 Pennsylvania South zone (US survey foot)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",39.3333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-77.75,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",40.9666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",39.9333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1968500,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United States (USA) - Pennsylvania - counties of Adams; Allegheny; Armstrong; Beaver; Bedford; Berks; Blair; Bucks; Butler; Cambria; Chester; Cumberland; Dauphin; Delaware; Fayette; Franklin; Fulton; Greene; Huntingdon; Indiana; Juniata; Lancaster; Lawrence; Lebanon; Lehigh; Mifflin; Montgomery; Northampton; Perry; Philadelphia; Schuylkill; Snyder; Somerset; Washington; Westmoreland; York.\"],\n        BBOX[39.71,-80.53,41.18,-74.72]],\n    ID[\"EPSG\",2272]]\n\nnrow(schools); nrow(crime); nrow(bike_network)\n\n[1] 495\n\n\n[1] 120567\n\n\n[1] 3719\n\n\nQuestions to answer: - What dataset did you choose and why? I selected three datasets, Philadelphia Schools, Crime Incidents, and the Philadelphia Bike Network for analyzing school safety environments. This combination allows an examination of whether schools are located in areas with high crime density and limited bicycle infrastructure, which is relevant for understanding walkability and student safety.\n\nWhat is the data source and date? All datasets were obtained from OpenDataPhilly. Schools: Data includes points identifying public, charter, private, and archdiocesan schools, as well as school annexes, athletic fields, and facilities. It supports the Streets Department’s school signage and crosswalk initiatives. Last updated February 2, 2024.\n\nCrime Incidents: Provided by the Philadelphia Police Department (2025), including Part I crimes such as aggravated assault, rape, and arson.\nBike Network – Supporting Datasets: Compiled by the Philadelphia City Planning Commission (April 2012), integrating data from multiple city departments to support bike network routing and documentation.\n\nHow many features does it contain? Schools: 495 point features Crime incidents: 120,567 point features Bike network: 3,719 line features\nWhat CRS is it in? Did you need to transform it? All datasets were standardized to EPSG:2272 – NAD83 / Pennsylvania South (ftUS). The Schools dataset was originally in WGS 84 (EPSG:4326) and was transformed to EPSG:2272 to ensure consistent spatial analysis and accurate distance or buffer measurements in feet.\n\n\n\nPose a research question\n\nResearch Question:Are schools in high-crime areas lacking nearby bicycle infrastructure?\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+): - Buffers - Spatial joins - Spatial filtering with predicates - Distance calculations - Intersections or unions - Point-in-polygon aggregation\nYour Task:\n\n# Create 500 ft buffers around schools\nschool_buffer &lt;- st_buffer(schools, dist = 500)\n\n# Create 300 ft buffers around bike lanes\nbike_buffer &lt;- st_buffer(bike_network, dist = 300)\n\n# Quick visualization\nlibrary(tmap)\n\ntmap_mode(\"plot\")\n\ntm_shape(bike_buffer) + \n  tm_borders(col = \"steelblue\", lwd = 1) +\n  tm_shape(school_buffer) + \n  tm_borders(col = \"red\", lwd = 1) +\n  tm_shape(crime) + \n  tm_dots(col = \"black\", size = 0.02) +\n  tm_layout(\n    title = \"School and Bike Lane Buffers with Crime Points\",\n    frame = FALSE,\n    legend.outside = TRUE\n  )\n\n\n\n\n\n\n\n\n\n# 1. Buffer\nschool_buffer &lt;- st_buffer(schools, dist = 500)\nbike_buffer &lt;- st_buffer(bike_network, dist = 300)\n\n# 2. Spatial join: count crimes in each school buffer\ncrime_in_school &lt;- st_join(crime, school_buffer, join = st_within)\n\ncrime_summary &lt;- crime_in_school %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(school_name) %&gt;%\n  summarise(crime_count = n())\n\n# 3. Identify whether each school has nearby bike lanes\nschool_bike_join &lt;- school_buffer %&gt;%\n  mutate(has_bike = if_else(\n    rowSums(st_intersects(., bike_buffer, sparse = FALSE)) &gt; 0, 1, 0\n  ))\n\n# 4. Merge crime + bike data\nschool_analysis &lt;- school_bike_join %&gt;%\n  left_join(crime_summary, by = \"school_name\") %&gt;%\n  mutate(crime_count = replace_na(crime_count, 0),\n         at_risk = if_else(crime_count &gt; 50 & has_bike == 0, 1, 0))\n\n# 5. Filter and visualize\n\nat_risk_schools &lt;- school_analysis %&gt;%\n  filter(at_risk == 1)\n\n# Optional: check how many schools\nnrow(at_risk_schools)\n\n[1] 52\n\nlibrary(tmap)\n\ntmap_mode(\"plot\")  # ✅ 静态输出模式\n\ntm_shape(bike_network) +\n  tm_lines(col = \"steelblue\", lwd = 1) +\n  tm_shape(school_buffer) +\n  tm_borders(col = \"firebrick\", lwd = 1) +\n  tm_shape(at_risk_schools) +\n  tm_dots(col = \"black\", size = 0.3) +\n  tm_layout(\n    title = \"At-Risk Schools: High Crime and No Nearby Bike Lanes\",\n    legend.outside = TRUE,\n    frame = FALSE\n  )\n\n\n\n\n\n\n\n\n\nschool_analysis_df &lt;- school_analysis %&gt;%\n  st_drop_geometry()\n\n# Summary\nsummary_table &lt;- school_analysis_df %&gt;%\n  summarise(\n    total_schools = n(),\n    total_at_risk = sum(at_risk, na.rm = TRUE),\n    avg_crime_near_school = round(mean(crime_count, na.rm = TRUE), 1)\n  ) %&gt;%\n  rename(\n    \"Total Schools\" = total_schools,\n    \"At-Risk Schools\" = total_at_risk,\n    \"Average Crimes Near Schools\" = avg_crime_near_school\n  )\n\nsummary_table %&gt;%\n  kbl(\n    caption = \"Table 1. Summary Statistics of School Safety and Bicycle Infrastructure in Philadelphia\",\n    align = c('c', 'c', 'c'),\n    booktabs = TRUE\n  ) %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n  )\n\n\n\nTable 1. Summary Statistics of School Safety and Bicycle Infrastructure in Philadelphia\n\n\nTotal Schools\nAt-Risk Schools\nAverage Crimes Near Schools\n\n\n\n\n495\n52\n1083.9\n\n\n\n\n\n# Drop geometry and clean data\nat_risk_table &lt;- at_risk_schools %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(school_name_label)) %&gt;%\n  select(\n    `School Name` = school_name_label,\n    `Crime Incidents` = crime_count,\n    `Nearby Bike Lane` = has_bike\n  ) %&gt;%\n  mutate(\n    `Crime Incidents` = formatC(`Crime Incidents`, format = \"d\", big.mark = \",\"),\n    `Nearby Bike Lane` = if_else(`Nearby Bike Lane` == 1, \"Yes\", \"No\")\n  ) %&gt;%\n  arrange(desc(as.numeric(`Crime Incidents`))) %&gt;%\n  slice_head(n = 10)\n\n# Output table\nat_risk_table %&gt;%\n  kbl(\n    caption = \"Table 2. Top 10 At-Risk Schools in Philadelphia: High Crime and No Nearby Bicycle Infrastructure\",\n    align = c(\"l\", \"c\", \"c\"),\n    booktabs = TRUE\n  ) %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n  ) %&gt;%\n  column_spec(1, bold = TRUE, width = \"5cm\") %&gt;%\n  column_spec(2:3, width = \"3cm\") %&gt;%\n  add_header_above(c(\" \" = 1, \"Safety Risk Indicators\" = 2))\n\n\n\nTable 2. Top 10 At-Risk Schools in Philadelphia: High Crime and No Nearby Bicycle Infrastructure\n\n\n\n\n\n\n\n\n\nSafety Risk Indicators\n\n\n\nSchool Name\nCrime Incidents\nNearby Bike Lane\n\n\n\n\nTECH FREIRE CHARTER SCHOOL\n138\nNo\n\n\nALLIANCE FOR PROGRESS CHARTER SCHOOL\n128\nNo\n\n\nALLIANCE FOR PROGRESS CHARTER SCHOOL (ANNEX)\n121\nNo\n\n\nISAAC A. SHEPPARD SCHOOL\n113\nNo\n\n\nONE BRIGHT RAY - FAIRHILL CAMPUS\n112\nNo\n\n\nYOUTHBUILD PHILADELPHIA CHARTER SCHOOL\n109\nNo\n\n\nGENERAL GEORGE G. MEADE SCHOOL\n100\nNo\n\n\nKIPP NORTH PHILADELPHIA ACADEMY\n95\nNo\n\n\nDEEP ROOTS CHARTER SCHOOL\n95\nNo\n\n\nPEOPLE FOR PEOPLE CHARTER SCHOOL\n91\nNo\n\n\n\n\n\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation: The analysis found that 52 out of 495 schools in Philadelphia are located in areas with both high crime density and no nearby bicycle infrastructure, posing potential safety concerns for students walking or biking to school.Most at-risk schools are concentrated in North and West Philadelphia, where crime incidents are more frequent. The absence of bicycle lanes near these schools may reduce safe commuting options for students and discourage active travel.Integrating safer bike routes and crime prevention strategies around schools could significantly improve student safety and accessibility."
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nI focused on simpler, cleaner map design and improved table formatting for easier interpretation."
  },
  {
    "objectID": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#submission-requirements",
    "href": "Assignments/Assignment_2/Yu_Xiao_Assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes",
    "section": "",
    "text": "[List main concepts from lecture] – Anscombe’s Quartet and the limits of summary statistics – Visualization in policy context – Connection to algorithmic bias and data ethics – ggplot2 fundamentals – Aesthetic mappings and geoms – Live demonstration – EDA workflow and principles – Understanding distributions and relationships – Critical focus: Data quality and uncertainty\n[Technical skills covered] ggplot(data = your_data) + aes(x = variable1, y = variable2) + geom_something() + additional_layers()"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3 Notes",
    "section": "",
    "text": "[List main concepts from lecture] – Anscombe’s Quartet and the limits of summary statistics – Visualization in policy context – Connection to algorithmic bias and data ethics – ggplot2 fundamentals – Aesthetic mappings and geoms – Live demonstration – EDA workflow and principles – Understanding distributions and relationships – Critical focus: Data quality and uncertainty\n[Technical skills covered] ggplot(data = your_data) + aes(x = variable1, y = variable2) + geom_something() + additional_layers()"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work] – Summary statistics can hide critical patterns – Outliers may represent important communities – Relationships aren’t always linear – Visual inspection reveals data quality issues"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I’ll apply this knowledge]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes",
    "section": "",
    "text": "[List main concepts from lecture] Repository (repo): Folder containing your project files Commit: Snapshot of your work at a point in time Push: Send your changes to GitHub cloud Pull: Get latest changes from GitHub cloud\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes",
    "section": "",
    "text": "[List main concepts from lecture] Repository (repo): Folder containing your project files Commit: Snapshot of your work at a point in time Push: Send your changes to GitHub cloud Pull: Get latest changes from GitHub cloud\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches] Powerful for spatial analysis and policy-focused statistics\n[Quarto features learned] Quarto is the “next generation” Better website creation Works with multiple programming languages Same basic concept, improved features"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work] Free and open source Excellent for spatial data Strong statistical capabilities Large community in urban planning/policy Reproducible research workflows"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I’ll apply this knowledge]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture] Repository (repo): Folder containing your project files Commit: Snapshot of your work at a point in time Push: Send your changes to GitHub cloud Pull: Get latest changes from GitHub cloud\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture] Repository (repo): Folder containing your project files Commit: Snapshot of your work at a point in time Push: Send your changes to GitHub cloud Pull: Get latest changes from GitHub cloud\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I’ll apply this knowledge]"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We observe data such as counties, income, population, and education.\nWe believe there’s some relationship between these variables.\nStatistical learning refers to the set of methods for estimating that relationship."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#the-general-problem",
    "href": "weekly-notes/week-06-notes.html#the-general-problem",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We observe data such as counties, income, population, and education.\nWe believe there’s some relationship between these variables.\nStatistical learning refers to the set of methods for estimating that relationship."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#formalizing-the-relationship",
    "href": "weekly-notes/week-06-notes.html#formalizing-the-relationship",
    "title": "Week 5 Notes",
    "section": "Formalizing the Relationship",
    "text": "Formalizing the Relationship\nFor any quantitative response Y and predictors X₁, X₂, … Xₚ:\n\\[\nY = f(X) + \\epsilon\n\\]\nWhere: - f = the systematic information X provides about Y\n- ε = random error (irreducible)"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#what-is-f",
    "href": "weekly-notes/week-06-notes.html#what-is-f",
    "title": "Week 5 Notes",
    "section": "What is f?",
    "text": "What is f?\nf represents the true relationship between predictors and the outcome.\nIt’s fixed but unknown — the core object we aim to estimate.\nExample: - Y = median income\n- X = population, education, poverty rate\n- f = how these factors systematically relate to income"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#why-estimate-f",
    "href": "weekly-notes/week-06-notes.html#why-estimate-f",
    "title": "Week 5 Notes",
    "section": "Why Estimate f?",
    "text": "Why Estimate f?\nTwo key purposes of statistical learning:\n\nPrediction\n\nEstimate Y for new or missing observations\n\nFocus on accuracy rather than explanation\n\nInference\n\nUnderstand how X affects Y\n\nIdentify which predictors are most important\n\nFocus on interpreting relationships"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#how-do-we-estimate-f",
    "href": "weekly-notes/week-06-notes.html#how-do-we-estimate-f",
    "title": "Week 5 Notes",
    "section": "How Do We Estimate f?",
    "text": "How Do We Estimate f?\n\nParametric Methods\n\nAssume a specific form for f (e.g., linear)\nReduce the problem to estimating parameters\nEasier to interpret but limited by assumptions\n\n\n\nNon-Parametric Methods\n\nMake fewer assumptions about f\nMore flexible but harder to interpret and require more data\n\nKey difference:\nParametric = structure first, then estimate\nNon-parametric = data first, let the shape emerge"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#parametric-approach-linear-regression",
    "href": "weekly-notes/week-06-notes.html#parametric-approach-linear-regression",
    "title": "Week 5 Notes",
    "section": "Parametric Approach: Linear Regression",
    "text": "Parametric Approach: Linear Regression\nWe assume a linear relationship between X and Y:\n\\[\nY ≈ β₀ + β₁X₁ + β₂X₂ + … + βₚXₚ\n\\]\nOur goal is to estimate the coefficients (β’s) using Ordinary Least Squares (OLS) — the method that minimizes prediction error."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#why-linear-regression",
    "href": "weekly-notes/week-06-notes.html#why-linear-regression",
    "title": "Week 5 Notes",
    "section": "Why Linear Regression?",
    "text": "Why Linear Regression?\nAdvantages - Simple and interpretable\n- Performs surprisingly well for many real-world problems\n- Foundation for more advanced methods\nLimitations - Assumes linearity and independence\n- Sensitive to outliers\n- Requires diagnostic checks"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#prediction-vs-inference",
    "href": "weekly-notes/week-06-notes.html#prediction-vs-inference",
    "title": "Week 5 Notes",
    "section": "Prediction vs Inference",
    "text": "Prediction vs Inference\nInference asks:\n&gt; Does education significantly affect income?\nFocus: understanding mechanisms, testing hypotheses.\nPrediction asks:\n&gt; What will the income be for this county?\nFocus: accuracy and reliability of forecasts.\nThis session emphasizes prediction, while recognizing inference as a complementary goal."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#example-applications",
    "href": "weekly-notes/week-06-notes.html#example-applications",
    "title": "Week 5 Notes",
    "section": "Example Applications",
    "text": "Example Applications\n\nPrediction\nUsed by governments to: - Estimate income for areas with incomplete census data\n- Forecast population growth or public service needs\nAccurate predictions improve decision-making even without full causal understanding.\n\n\nInference\nUsed by researchers to: - Understand gentrification and inequality\n- Identify which neighborhood characteristics explain income changes\n- Evaluate the impact of education or policy interventions"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#connection-to-week-2-algorithmic-bias",
    "href": "weekly-notes/week-06-notes.html#connection-to-week-2-algorithmic-bias",
    "title": "Week 5 Notes",
    "section": "Connection to Week 2: Algorithmic Bias",
    "text": "Connection to Week 2: Algorithmic Bias\nStatistical accuracy does not guarantee ethical fairness.\nFor example, a healthcare model once predicted medical needs using costs as a proxy — resulting in racial bias.\nThis highlights that fit metrics (like R²) cannot replace ethical evaluation."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#interpreting-regression-results",
    "href": "weekly-notes/week-06-notes.html#interpreting-regression-results",
    "title": "Week 5 Notes",
    "section": "Interpreting Regression Results",
    "text": "Interpreting Regression Results\nWhen fitting a simple model of median income vs. population: - The intercept (~$62,855) represents baseline income when population = 0 (not practically meaningful).\n- The slope (~$0.02 per person) means that, on average, income rises $20 for every additional 1,000 people.\n- The relationship is statistically significant (p &lt; 0.001).\n- R² = 0.21 → about 21% of variation in income is explained by population.\nThis shows population helps explain income differences, though other variables matter too."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#model-evaluation-key-ideas",
    "href": "weekly-notes/week-06-notes.html#model-evaluation-key-ideas",
    "title": "Week 5 Notes",
    "section": "Model Evaluation: Key Ideas",
    "text": "Model Evaluation: Key Ideas\n\nThe “True” vs. Estimated Relationship\nThe real relationship (f) is unobservable.\nOur model provides an approximation based on available data.\nEach dataset produces slightly different estimates — uncertainty is measured by standard errors.\n\n\nStatistical Significance\n\nNull hypothesis (H₀): β₁ = 0 (no relationship)\n\nA small p-value means the observed effect is unlikely to be random → the relationship is real.\n\n\n\nR² and Model Fit\n\nR² measures the share of variance in Y explained by X.\n\nIt indicates strength, not correctness, of the model.\n\nA model with low R² may still provide useful predictions."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#overfitting-and-model-validation",
    "href": "weekly-notes/week-06-notes.html#overfitting-and-model-validation",
    "title": "Week 5 Notes",
    "section": "Overfitting and Model Validation",
    "text": "Overfitting and Model Validation\nA model can perform well on training data but fail on new data.\nTo avoid this, data are split into training and testing sets.\n\nTraining error measures fit on known data.\n\nTesting error measures predictive performance on unseen data.\n\nGood models balance bias and variance, avoiding both underfitting and overfitting."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#model-assumptions-and-diagnostics",
    "href": "weekly-notes/week-06-notes.html#model-assumptions-and-diagnostics",
    "title": "Week 5 Notes",
    "section": "Model Assumptions and Diagnostics",
    "text": "Model Assumptions and Diagnostics\n\nLinearity — relationship between variables should be roughly linear.\n\nCheck with residual plots.\n\nCurvature suggests model misspecification.\n\nConstant Variance (Homoscedasticity) — residual spread should be even.\n\nViolations make p-values unreliable.\n\nSolutions: transform Y, add predictors, or use robust errors.\n\nNormality of Residuals — important for hypothesis testing.\n\nUse Q–Q plots to assess normal distribution.\n\nNo Multicollinearity — predictors should not be highly correlated.\n\nHigh correlation makes coefficients unstable.\n\nNo Influential Outliers — avoid points that disproportionately shape the regression line.\n\nDetect using Cook’s Distance."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#improving-the-model",
    "href": "weekly-notes/week-06-notes.html#improving-the-model",
    "title": "Week 5 Notes",
    "section": "Improving the Model",
    "text": "Improving the Model\n\nAdding Predictors\nIncluding education and poverty rates typically improves explanatory power.\nExample: Adjusted R² increases to around 0.57, suggesting stronger fit.\n\n\nTransformations\nIf relationships are curved, use log transformations.\nFor instance, a log–population model may capture diminishing returns to scale.\n\n\nCategorical Variables\nInclude binary variables such as metro vs. non-metro areas.\nIn one model, metro counties earned about $30,000 more on average."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#summary-the-regression-workflow",
    "href": "weekly-notes/week-06-notes.html#summary-the-regression-workflow",
    "title": "Week 5 Notes",
    "section": "Summary: The Regression Workflow",
    "text": "Summary: The Regression Workflow\n\nUnderstand the conceptual model (f(X)).\n\nVisualize relationships before modeling.\n\nFit the regression model carefully.\n\nEvaluate performance using validation methods.\n\nCheck key assumptions.\n\nRefine with new variables or transformations.\n\nReflect on the ethical and social implications of modeling."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#key-takeaways",
    "href": "weekly-notes/week-06-notes.html#key-takeaways",
    "title": "Week 5 Notes",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nStatistical learning is about estimating f(X), the systematic link between predictors and outcomes.\n\nTwo central goals:\n\nInference – understand relationships\n\nPrediction – forecast new outcomes\n\n\nGood fit ≠ good model: always assess assumptions and fairness.\n\nDiagnostics matter: plots and reasoning reveal what statistics can hide.\n\nEthical awareness is essential in all modeling decisions."
  }
]